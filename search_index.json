[
["index.html", "Community contributions for EDAV Fall 2019 Chapter 1 Instructions 1.1 Background 1.2 Preparing your .Rmd file 1.3 Submission Steps 1.4 Optional tweaks 1.5 FAQ", " Community contributions for EDAV Fall 2019 2019-11-03 Chapter 1 Instructions This chapter gives you all the information you need to upload your community contribution. Please read this entire document carefully before making your submission. Of particular note is the fact that bookdown requires a different .Rmd format than you’re used to, so you must make changes to the beginning of the file as described below before submitting. 1.1 Background This web site makes use of the bookdown package to render a collection of .Rmd files into a nicely formatted online book with chapters and subchapters. Your job will be to submit a slightly modified version of your community contribution .Rmd file to the GitHub repository where the source files for this web site are stored. On the backend, the admins will divide the chapters into book sections and order them. We use Travis CI to render the book and push the rendered .html files to our gh-pages branch–you can view our builds here–and GitHub Pages to host the site. If your community contribution is in a different format, then create a short .Rmd file that explains what you did, and includes links to any relevant files, such as slides, etc. which you can post on your GitHub repo (or another online site.) 1.2 Preparing your .Rmd file You should only submit ONE Rmd file. After completing these modifications, your .Rmd should look like this sample bookdown .Rmd. Create a concise, descriptive name for your project. For instance, name it base_r_ggplot_graph or something similar if your work is about contrasting/working with base R graphics and ggplot2 graphics. Check the .Rmd filenames in the project repo to make sure your name isn’t already taken. Your project name should be words only and joined with underscores, i.e. Do not include whitespace in the name. Create a copy of your .Rmd file with the new name. Completely delete the YAML header (the section at the top of the .Rmd that includes name, title, date, output, etc.) including the --- line. Choose a short, descriptive, human readable title for your project as your title will show up in the table of contents – look at examples in the rendered book. Capitalize the first letter only (“sentence case”). On the first line of your document, enter a single hashtag, followed by a single whitespace, and then your title. It is important to follow this format so that bookdown renders your title as a header. Do not use single # headers anywhere else in the document. The second line should be blank, followed by your name(s): # Base R vs. ggplot2 Aaron Burr and Alexander Hamilton Your content starts here. If your project requires data, please use a built-in dataset or read directly from a URL, such as: df &lt;- readr::read_csv(&quot;https://people.sc.fsu.edu/~jburkardt/data/csv/addresses.csv&quot;) If you absolutely must include a data file, please use a small one, as for many reasons it is desirable to keep the repository size as small as possible. If you have included a setup chunk in your Rmd file, please remember to remove the label setup in the chunk, ie., use : {r, include=False} instead of {r setup, include=False} Want to get fancy? See the optional tweaks section below. 1.3 Submission Steps To submit your work, we will be following the instructions in this tutorial, which are provided in abbreviated form below, with specific instructions on naming conventions, content information, and other important details. Fork cc19 repo (this repo) to your GitHub account. Clone/download the forked repo to your local computer. Create a new branch and name it with your project name, in our case sample_project. If you forget to do so, check this tutorial to fix. Copy your modified .Rmd file with the same name into the root directory on the branch. In our example, it is sample_project.Rmd. Do not include an .html file. (In order for the bookdown package to work, all .Rmd files will be rendered behind the scenes.) [OPTIONAL] If you have other resources (such as images) included in your project, create a folder under resources/. In our example, it is resources/sample_project/. Put the resources files there. When you are ready to submit your project, push your branch to your remote repo. Follow this tutorial to create a pull request. If you follow the steps, we will merge it to the master branch. After submitting your pull request, do not be concerned if you see an “All builds have failed” message from Travis CI. There are things that need to be done on the backend, such as adding the libraries you use to the project for the Travis CI build to pass. 1.4 Optional tweaks If you prefer for links from your chapter to open in new tabs, add {target=&quot;_blank&quot;} after the link, such as: [edav.info](edav.info){target=&quot;_blank&quot;} Note that your headers (##, ###, etc.) will be converted to numbered headings as such: ## –&gt; 3.1 ### –&gt; 3.1.1 These headings will appear as chapter subheadings and sub-subheadings in the navigation panel on the left. Think about a logical structure for users to navigate your chapter. We recommend using only ## and ### headings as subheadings such as 4.1.3.4 are generally not necessary and look messy. Unfortunately, there’s no simple way to preview your chapter before it’s actually merged into the project. (bookdown has preview_chapter() option but it only works after the entire book has been rendered at least once and that will become more and more complex and require more and more packages as the project grows.) If you really want to preview it, fork and clone this minimal bookdown repo, add your .Rmd file, click the “Build book” button on the Build tab (next to Git), and then open any of the .html files in the _book folder in a web browser to see the rendered book. (Do not click the Knit button as it will not build a bookdown book.) If you’re interested in more bookdown options, see the official reference book. Have more useful tweaks to share? Submit an issue or PR. 1.5 FAQ 1.5.1 What should I expect after creating a pull request? Within a week after you create a pull request, we will apply a label to it and assign an administrater who will review all the files you submit to see if they meet the requirements. It will take some time before we can process all the pull requests, so as long as you see your pull request has been labeled and assigned to an administrater, don’t worry. However, if the admin contacts you regarding the pull request, that usually means your files fail to meet some requirements. The admin will clearly state what is wrong, so please fix them as soon as possible. 1.5.2 What if I catch mistakes after my pull request is merged? You may submit additional pull requests to fix material on the site. If the edits are small, such as fixing typos, it is easiest to make the edits directly on GitHub, following these instructions. We will merge first pull requests before edits, so please be patient. 1.5.3 Other questions If you encounter other problems, please submit an issue and we will look into it. Thank you for your contributions! "],
["sample-project.html", "Chapter 2 Sample project", " Chapter 2 Sample project Nancy Pelosi and Donald Trump This chapter gives a sample layout of your Rmd file. Test Photo "],
["stamen-maps-with-ggmap.html", "Chapter 3 Stamen maps with ggmap 3.1 Mutilayerd plots with ggmaps 3.2 Getting Deeper", " Chapter 3 Stamen maps with ggmap Mrugank Akarte Here is an example to get started with ggmap using get_stamenmap() to plot the longitude/latitude maps. The data for the following plots is available at https://simplemaps.com/data/us-cities. The get_stamenmap() function reqiures a bounding box, i.e the top, bottom, left and right latitude/longitude of the map you want to plot. For example, the latitude/longitude for US map are as follows: bbox &lt;- c(bottom = 25.75, top = 49 , right = -67, left = -125) You can find these values from https://www.openstreetmap.org. The other important parameters of this function are zoom and maptype. Higher the zoom level, the more detailed your plot will be. Beaware that ggmap connects to Stamen Map server to download the map, so if your bounding box is large and zoom level is high, it will have to download a lot of data and may take some time. There are differnt types of plots available via Stamen Map like terrain, watercolor, toner which can be set to maptype parameter according to your preference. You can find about avaiable options in help (?get_stamenmap). For the following examples the maptype is set to ‘toner-lite’. Let’s plot the US map. library(ggmap) usmap &lt;- get_stamenmap(bbox = bbox, zoom = 6, maptype = &#39;toner-lite&#39;) ggmap(usmap) Great! We have the US map, now let’s use the US population data to see the spread of counties across nation. Notice that we haven’t included Alaska in the map and hence will be removing the data from Alaska. library(dplyr) df &lt;- read.csv(unz(&#39;resources/ggmap/data/uscities.zip&#39;, &#39;uscities.csv&#39;)) # Removing data of Alaska from dataset df &lt;- df %&gt;% filter(state_name != &#39;Alaska&#39;) # Spread of counties across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Spread of counties across US&#39;) This is not good! Most of the points are overlapping and thus it is not easy to interpret what’s going on here. Let’s try alpha blending and reduce the size of points. # Spread of counties across US using points ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population), size = 0.8, stroke= 0, alpha = 0.4) + ggtitle(&#39;Spread of counties across US&#39;) That’s much better! We can now easily identify the areas where number of counties are more. You might have noticed there is no light blue dot visible on the plot. This is because it must be lying somewhere between those dense areas. One such location is New York, you can find this out by zooming the plot. Another reason is that when you use alpha blending, your colors fade and thus it becomes difficult to identify such points. We can also look at spread of counties using geom_density as follows # spread of counties across US using Density_2d ggmap(usmap) + geom_density_2d(data = df, mapping = aes(x = lng, y = lat, color = population)) + ggtitle(&#39;Spread of counties across US&#39;) 3.1 Mutilayerd plots with ggmaps We can add multiple layers to the plot as described in earlier chapters. Let’s look at the location of military stations located across US along with population density. # Location of Military units df1 &lt;- df %&gt;% filter(military == TRUE) ggmap(usmap) + geom_point(data = df, mapping = aes(x = lng, y = lat, color = population, text = city), show.legend = F, size = 0.8, stroke= 0, alpha = 0.4) + geom_point(data = df1, mapping = aes(x = lng, y = lat , text = city), show.legend = F, size = 0.9, color = &#39;red&#39;) + ggtitle(&#39;Military stations across US&#39;) As you can see, there are 3 layers in this plot. First base layer consists of US map, second layer consists of spread of counties across US and the third layer consists of location of military bases. It is not easy to plot such multilayered graphs using other packages. Let’s zoom the map for state of California and see some other map types offered by Stamen Maps. # California Boundaries par(mfrow=c(3,1)) CAbox &lt;- c(bottom = 32.213, top = 42.163 , right = -113.95, left = -124.585) camap1 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;watercolor&#39;) camap2 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;terrain&#39;) camap3 &lt;- get_stamenmap(bbox = CAbox, zoom = 6, maptype = &#39;toner-hybrid&#39;) ggmap(camap1) ggmap(camap2) ggmap(camap3) 3.2 Getting Deeper This was just a glimpse of what you can do with ggmaps using the get_stamenmap(). Note that Stamen Maps is not limited to US and can be used to plot any part of the world. If you liked this alternative to Google Maps API, I highly recommend you to check the Stamen Maps website http://maps.stamen.com for more details. "],
["ridgeline-plots.html", "Chapter 4 Ridgeline plots 4.1 Overview 4.2 tl;dr 4.3 Simple examples 4.4 Theory 4.5 External resources", " Chapter 4 Ridgeline plots Hojin Lee and Hyuk Joon Kwon 4.1 Overview Ridgeline plot is a set of overlapped density plots, and it helps us to compare multiple distirbutions among dataset. Professor Claus O. Wilke from UT Austin, who created ggridges package, commented about ridgeline plot as below: “Ridgeline plots are partially overlapping line plots that create the impression of a mountain range. They can be quite useful for visualizing changes in distributions over time or space. These types of plots have also been called “joyplots”, in reference to the iconic cover art for Joy Division’s album Unknown Pleasures. However, given the unfortunate origin of the name Joy Division, the term “joyplot” is now discouraged.&quot; In this section, we will discuss how to create ridgeline plots using the ggplot and ggridges libraries. 4.2 tl;dr For those who do not want to go through the documents, the below is the polished version of a ridgeline plot and the codes. library(ucidata) library(ggplot2) library(ggridges) library(viridis) library(plyr) library(nycflights13) weather$month &lt;- as.factor(weather$month) ggplot(weather, aes(x = temp, y = reorder(month, desc(month)), fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom = &quot;density_ridges_gradient&quot; , alpha = 0.6 , scale = 2.3) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) + ggtitle(&quot;What is the weather like in NYC?&quot;, subtitle = &quot;Ridgeline plot for NYC temperature by months&quot;) + xlab(&quot;Temperature (F)&quot;) + ylab(&quot;Months&quot;) + labs(caption = &quot;Source: nycflights13::weather&quot;) + theme(plot.title = element_text(face=&quot;bold&quot;)) + theme(plot.subtitle = element_text(face=&quot;bold&quot;, color=&quot;grey&quot;)) + theme(plot.caption=element_text(color=&quot;grey&quot;)) ## Picking joint bandwidth of 1.58 ## Warning: Removed 1 rows containing non-finite values (stat_density_ridges). For more information about dataset, type ?nycflights13::weather into the console. 4.3 Simple examples For one who needs friendly step by step approach, please read the below. First, we need to install ggridges and ggplot2 packages. #install.packages(&quot;ggridges&quot;) #install.packages(&quot;ggplot2&quot;) Make sure that the y variable is a categorical variable, otherwise the function will throw an error. You can use y = as.factor(data) to transfrom your y variable into a categorical variable. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges() ## Picking joint bandwidth of 21 If you do not want the ridgeline plot to touch each other, please use the scale variable. A scale of 1.0 will make the adjust graph to barely touch each other. If the scale is greater than 1 the graphs will overlap with each other. Otherwise, if the scale is less than 1 the graphs will not touch each other. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = DMC, y = day)) + geom_density_ridges(scale = 1.1) ## Picking joint bandwidth of 21 There is a raindrop function within ridgeline plots, which combine the rideline plots with scatter plots; the function will plot scatter plot under the rideline plot. library(ISwR) data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation)) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) ## Picking joint bandwidth of 24.5 Morevoer, it is possible to divide the data into quantiles and draw lines in between. This way, it would be easier for us to observe the median value and the interquartile range. data &lt;- forest_fires data$day &lt;- factor(data$day , levels= rev(c(&quot;sun&quot;, &quot;mon&quot;, &quot;tue&quot;, &quot;wed&quot;, &quot;thu&quot;, &quot;fri&quot;, &quot;sat&quot;))) ggplot(data, aes(x = temp, y = day, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines =TRUE , geom=&quot;density_ridges_gradient&quot;) + scale_fill_viridis(discrete = TRUE , name = &quot;Quantile&quot; , option = &quot;plasma&quot;) ## Picking joint bandwidth of 1.93 In below, we have merged all the functions we have introduced, and here is the result! data2 &lt;- red.cell.folate ggplot(data2, aes(x = folate, y = ventilation, fill = factor(..quantile..))) + stat_density_ridges(quantiles = c(0.25,0.5,0.75) , quantile_lines = TRUE , geom=&quot;density_ridges_gradient&quot; , jittered_points = TRUE , position = &quot;raincloud&quot; , alpha = 0.6 , scale = 0.6) + scale_fill_viridis(discrete=TRUE , name = &quot;Quantile&quot; , alpha = 0.3 , option = &quot;cividis&quot;) ## Picking joint bandwidth of 24.5 Here is one last cool feature of ridgeline plots where we can overlap distributions within same data group. This enables us to compare distributions not only among different data groups but also within same data groups. library(vcd) data3 &lt;- Arthritis ggplot(data3) + geom_density_ridges(aes(x = Age, y = Treatment, group = interaction(Treatment,Improved),fill = Improved), alpha = 0.7) ## Picking joint bandwidth of 5.42 4.4 Theory Ridgeline plots are an overlap of histograms over y-axis, and this allows us to visualize and compare overall shape of distribution among different groups. They work very well when the dataset has high number of groups to show. Also, since we are overlapping distributions, we can save space for graphs. In other words, if the number of groups to represent is too small, plotting ridgeline plots might not be an optimal choice for data visualization. On the other hand, the ridgeline plots work well when there are clear differences in distributions. Otherwise, because of overlaps, they would cause more confusion when deciphering the data. Couple points to think about before plotting the ridgeline plots: Ordering of groups will change overall shape of the plots. Figure out the optimal bin size &amp; bandwidth argument for the visualization. 4.5 External resources The below has more examples with the ridgeline plots: https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html https://cmdlinetips.com/2018/03/how-to-plot-ridgeline-plots-in-r/ "],
["gantt-charts.html", "Chapter 5 Gantt charts 5.1 Using geom_line 5.2 Using the package ‘plan’", " Chapter 5 Gantt charts Phani Kumar Valasa and Harish Babu Visweswaran A Gantt chart is a visual view of tasks scheduled over time. Gantt charts are used for planning projects and are a useful way of showing what work is scheduled to be done on a specific day. The other elements Gantt charts may provide include the start and end dates of a task/project, the owner(s) of each task, task dependencies, completion status, task grouping and more. In this write-up, we will be focusing only on some of those elements. This can be a useful tutorial for students interested in representing their project plans (think Final Project). We talk about two ways to plot Gantt Charts in R - one using geom_line and playing with the aesthetics and the other using a package called plan. We will compare the two methods in this brief tutorial. Let’s jump right into it. We first load the necessary packages for plotting a Gantt Chart. We actually just need to load the tidyverse package for the plot using geom_line. That will in turn load ggplot2, forcats and all other packages we need. In addition, we need the plan package for the second part of the tutorial. # Loading the necessary pacakges library(tidyverse) library(plan) 5.1 Using geom_line We clearly need to have our data ready before we learn how to make the plot. For the first part of this tutorial, we assume the data is in a dataframe. We can either import the data from an excel or a csv or we can just create the dataframe explicitly inside R if you have a manageable number of tasks. Let’s create a dataframe inside R for this tutorial. df &lt;- data.frame(task=c(&quot;Explore Ideas&quot;, &quot;Finalize Idea&quot;, &quot;Make Plots with\\n geom_line&quot;, &quot;Make Plots with\\n Plan Package&quot;, &quot;Add Writeups&quot;, &quot;Refine Plots&quot;, &quot;Review Tutorial&quot;, &quot;Submit Tutorial&quot;, &quot;Get Feedback\\n and Update&quot;), start=c(&quot;2019-10-15&quot;, &quot;2019-10-18&quot;, &quot;2019-10-19&quot;, &quot;2019-10-22&quot;, &quot;2019-10-24&quot;, &quot;2019-10-25&quot;, &quot;2019-10-26&quot;, &quot;2019-10-27&quot;, &quot;2019-10-28&quot;), end=c(&quot;2019-10-18&quot;, &quot;2019-10-19&quot;, &quot;2019-10-24&quot;, &quot;2019-10-26&quot;, &quot;2019-10-27&quot;, &quot;2019-10-27&quot;, &quot;2019-10-28&quot;, &quot;2019-10-28&quot;, &quot;2019-10-31&quot;), owner=c(&quot;Harish&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;, &quot;Harish&quot;, &quot;Phani&quot;)) Printing the first few rows of the dataframe: head(df) ## task start end owner ## 1 Explore Ideas 2019-10-15 2019-10-18 Harish ## 2 Finalize Idea 2019-10-18 2019-10-19 Phani ## 3 Make Plots with\\n geom_line 2019-10-19 2019-10-24 Harish ## 4 Make Plots with\\n Plan Package 2019-10-22 2019-10-26 Phani ## 5 Add Writeups 2019-10-24 2019-10-27 Phani ## 6 Refine Plots 2019-10-25 2019-10-27 Harish Converting the dates from factor type to date type: df &lt;- df %&gt;% mutate(start = as.Date(start), end = as.Date(end)) YYYY-MM-DD is the preferred format for dates but if you have other formats, you can still use the as.Date function to convert them to dates by passing in the tryFormats argument as a vector Ex: tryFormats = c(“%Y-%m-%d”, “%Y/%m/%d”) will check if the Date fits any of the passed formats. In order for us to be able to use the line plot, we need to tidy the data - the start and the end date need to be in the same column. We will use the gather function from tidyr to transform the dataframe. We add another column that indicates whether the date is the start date or the end date. df_tidy &lt;- df %&gt;% gather(key=date_type, value=date, -task, -owner) The trick in using a line plot for the Gantt chart is to make the line very thick so that it looks like a bar. If we then have a start and end value for the bar, we should be able to get what we need. We will flip the coordinates so that the bars are horizontal (which is more in line with how a Gantt chart looks). We will start with a basic plot using geom_line and then make updates until we are happy with the plot. To begin with, we will plot the task name on the x-axis and the date on the y-axis and then flip the coordinates. We will adjust the size parameter to get a bar instead of a line ggplot() + geom_line(data=df_tidy, mapping=aes(x=task, y=date), size=10) + coord_flip() The plot is directionally fine but we observe a few issues. The tasks are not in the order we entered them in. We can fix that by using the fct_inorder function from the forcats package (the task field is a factor and forcats provides many functions that help with factor ordering). Since we are dealing with a horizontal line plot, the plot starts from the bottom (the first value passed will be at the bottom and the last value passed will be at the top). Since we want to read out plot from the top, we will reverse the order using fct_rev. Additionally, we want to indicate the owner of each task. We will do that by passing the color argument to geom_line. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + coord_flip() Our plot looks pretty good now. Gantt charts optionally have vertical line markers that indicate the current date. This serves as a frame of reference for members to evaluate whether they are on track to complete the project in time. Let’s add this using geom_hline before the coordinate flip. Let’s use a dashed black line to represent this. Normally we would use the Sys.Date() function from base R to get the current date but here we hardcode the date to 2019-10-27, so that rerunning the code at a later date will still display the line. We will also add some labels. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) We now have a version of the Gantt Chart that we can use. We will now tidy up the graph by updating the date ticks to make them more frequent and lining up the grid lines with the labeled dates. We will also update the theme to the bw theme to get a cleaner look. ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + scale_y_date(date_breaks = &quot;1 day&quot;) + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90), panel.grid.minor = element_line(colour=&quot;white&quot;, size=0.5), legend.position=&quot;right&quot;, plot.title = element_text(hjust = 0.5)) Finally, let’s say we want a less instrusive way to indicate that a certain task has been completed. One option is to make the completed bars transparent and the let the incomplete bars be the way they are. For this, we need to have the status of the task in another column. Let’s add a column named completed to the original dataframe and enter values of 1 for completed tasks and 0 for incomplete tasks. We will then use the completed column inside the aesthetics as the argument for the alpha parameter. We set alpha to 0.2 for incomplete tasks and 1 for complete tasks using the scale_alpha_discrete function. Additionally, we get a legend that is not super helpful - we remove it by passing guide=“none” inside the scale_alpha_discrete function. This is not the best possible way to visualize the status but it works. df_completed &lt;- df %&gt;% mutate(completed = factor(c(rep(1, 6), rep(0, 3)))) df_tidy &lt;- df_completed %&gt;% gather(key=date_type, value=date, -task, -owner, -completed) ggplot() + geom_line(data=df_tidy, mapping=aes(x=fct_rev(fct_inorder(task)), y=date, color=owner, alpha=completed), size=10) + geom_hline(yintercept=as.Date(&quot;2019-10-27&quot;), colour=&quot;black&quot;, linetype=&quot;dashed&quot;) + coord_flip() + scale_alpha_discrete(range=c(1, 0.2), guide=&quot;none&quot;) + scale_y_date(date_breaks = &quot;1 day&quot;) + labs(title=&quot;Community Contribution Gantt Chart&quot;, x = &quot;Task&quot;, y = &quot;Date&quot;, colour = &quot;Owner&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90), panel.grid.minor = element_line(colour=&quot;white&quot;, size=0.5), legend.position=&quot;right&quot;, plot.title = element_text(hjust = 0.5)) 5.2 Using the package ‘plan’ Now we show how to create a Gantt chart using the ‘plan’ package. It has predefined methods to read the data and plot the gantt chart. Here is the reference to the package for more details: https://cran.r-project.org/web/packages/plan/plan.pdf The data used is similar to the one we used for creating the charts using geom_line. We’ll be showing two ways to read the data. One is adding the tasks manually to the ‘gantt’ object by using the ‘ganttAddTask’ method. The other is to read the tasks from an existing file. Let’s explore the first method of adding tasks via ‘ganttAddTask’ in our tutorial project. This method takes the task description, start date, end date and percentage completion as input. As you see below, we need to add the percentage completion of the task as with any project planning. The ‘plot’ takes care of creating a Gantt chart from this object. g &lt;- new(&quot;gantt&quot;) g &lt;- ganttAddTask(g, &quot;Explore Ideas&quot;,&quot;2019-10-15&quot;,&quot;2019-10-18&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Finalize project&quot;,&quot;2019-10-18&quot;,&quot;2019-10-19&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Create WBS&quot;,&quot;2019-10-19&quot;,&quot;2019-10-20&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - geom_line&quot;,&quot;2019-10-20&quot;,&quot;2019-10-24&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - package&quot;,&quot;2019-10-22&quot;,&quot;2019-10-26&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Add Writeup&quot;,&quot;2019-10-24&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Refine Plot&quot;,&quot;2019-10-25&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Review Tutorial&quot;,&quot;2019-10-26&quot;,&quot;2019-10-28&quot;,done=80 ) g &lt;- ganttAddTask(g, &quot;Submit Tutorial&quot;,&quot;2019-10-27&quot;,&quot;2019-10-28&quot;,done=0 ) g &lt;- ganttAddTask(g, &quot;Get Feedback and Update&quot;,&quot;2019-10-28&quot;,&quot;2019-10-31&quot;,done=0 ) plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), event.time=&quot;2019-10-27&quot;, event.label=&quot;Report Date&quot;, main = &quot;Community Contribution Gantt Chart&quot;) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=gray(c(0.3, 0.9)), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) In addition to the above, using the package, we are also able to categorize sections of tasks into groups. Here, we have split the tasks into groups named Plan, Implement and Review for demonstration. The interface is simple - we just need to include a task with just the name of the task. g &lt;- new(&quot;gantt&quot;) g &lt;- ganttAddTask(g, &quot;Plan&quot;) g &lt;- ganttAddTask(g, &quot;Explore Ideas&quot;,&quot;2019-10-15&quot;,&quot;2019-10-18&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Finalize project&quot;,&quot;2019-10-18&quot;,&quot;2019-10-19&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Create WBS&quot;,&quot;2019-10-19&quot;,&quot;2019-10-20&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Implement&quot;) g &lt;- ganttAddTask(g, &quot;Gantt chart - geom_line&quot;,&quot;2019-10-20&quot;,&quot;2019-10-24&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Gantt chart - package&quot;,&quot;2019-10-22&quot;,&quot;2019-10-26&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Add Writeup&quot;,&quot;2019-10-24&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Refine Plot&quot;,&quot;2019-10-25&quot;,&quot;2019-10-27&quot;,done=100 ) g &lt;- ganttAddTask(g, &quot;Review&quot;) g &lt;- ganttAddTask(g, &quot;Review Tutorial&quot;,&quot;2019-10-26&quot;,&quot;2019-10-28&quot;,done=80 ) g &lt;- ganttAddTask(g, &quot;Submit Tutorial&quot;,&quot;2019-10-27&quot;,&quot;2019-10-28&quot;,done=0 ) g &lt;- ganttAddTask(g, &quot;Get Feedback and Update&quot;,&quot;2019-10-28&quot;,&quot;2019-10-31&quot;,done=0 ) plot(g, ylabel=list(font=ifelse(is.na(g[[&quot;start&quot;]]), 2, 1)), event.time=&quot;2019-10-27&quot;, event.label=&quot;Report Date&quot;, main = &quot;Community Contribution Gantt Chart&quot;) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=gray(c(0.3, 0.9)), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) The below is the second method of reading the tasks from a file. Here is the format of the file used, for it be read by ‘read.gantt’ method. Key,Description,Start,End,Done,NeededBy 1,Explore Ideas,2019-10-15,2019-10-18,100, 2,Finalize project,2019-10-18,2019-10-19,100, 3,Create WBS,2019-10-19,2019-10-20,100, 4,Gantt chart - package,2019-10-20,2019-10-24,100, 5,Gantt chart - geom_line,2019-10-22,2019-10-26,100, 6,Add Writeup,2019-10-24,2019-10-27,100, 7,Refine Plot,2019-10-25,2019-10-27,100, 8,Review Tutorial,2019-10-26,2019-10-28,80, 9,Submit Tutorial,2019-10-27,2019-10-28,0, 10,Get Feedback and Update,2019-10-28,2019-10-31,0, We can also add the dependency on other tasks in the above file using the NeededBy column but we haven’t used it in this example (we’ve left it blank). However there is no provision to add the owner of the task/resource who will be working on the task. We’ve explored this in creation of gantt_chart with ‘geom_line’. Once the data is read using ‘read.gantt’ method, the output object is used by ‘plot’ to create the Gantt chart. You’ll notice the code is simple and doesn’t look like a hack to produce the chart. Note that the file is hosted on github - so running the below script as is should work. gt_object &lt;- read.gantt(&quot;https://raw.githubusercontent.com/harish-cu/cc19/tasks_file/tasks.csv&quot;) plot(gt_object,event.label=&#39;Report Date&#39;,event.time=&#39;2019-10-27&#39;, col.event=c(&quot;red&quot;), col.done=c(&quot;lightblue&quot;), col.notdone=c(&quot;pink&quot;), main=&quot;Community Contribution Gantt Chart&quot; ) legend(&quot;topright&quot;, pch=22, pt.cex=2, cex=0.9, pt.bg=c(&quot;lightblue&quot;, &quot;pink&quot;), border=&quot;black&quot;, legend=c(&quot;Completed&quot;, &quot;Not Yet Done&quot;), bg=&quot;white&quot;, xpd=TRUE) From the above, as you see, it is easy to plot a Gantt chart using the package. This provides an easy way of looking at what has been completed and pending. It calculates the proportion of the line to color from the percentage completion we provide. However it doesn’t have the ability to color/segment the chart based on the resources working on those tasks. The annotations on the chart (from ‘plot’) needs a little work as the labelling is not as straight forward compared to what we get with geom_line. And any customizations on the Gantt chart such as segmenting the chart by ‘Resource’, providing an effort based chart rather than duration etc., needs significant work and may not be directly produced by this package. As long as the output meets your requirements and the tasks can be maintained in the format required by this package, ‘plan’ provides an excellent solution to produce quick Gantt charts. However if your requirements needs additional customizations, any custom solution built using geom_line would provide a better alternative. Note: There are other packages like Candela and DiagrammeR that we have not explored in this tutorial but might be of interest as they provide similar capabilities to draw Gantt Charts Sources: https://www.molecularecologist.com/2019/01/simple-gantt-charts-in-r-with-ggplot2-and-the-tidyverse/ https://cran.r-project.org/web/packages/plan/plan.pdf "],
["likert.html", "Chapter 6 Likert 6.1 Overview 6.2 tl;dr 6.3 Simple examples 6.4 Stacked bar chart using ggplot 6.5 Theory 6.6 When to use 6.7 External resources", " Chapter 6 Likert Shijie He and Chutian Chen 6.1 Overview This section covers how to make stacked bar chart on likert data. Likert data is the data with likert scale. Likert scale is a several point scale which is used to allow people to express how much they agree or disagree with a particular statement. And It’s commonly used in survey and research. 6.2 tl;dr Here’s a stacked bar chart of angry levels: And here’s the code: library(HH) library(dplyr) # create data data = data.frame(&quot;Not_at_all_angry&quot;=c(0.11,0.08,0.09,0.08,0.09,0.12,0.05,0.08),&quot;Not_very_angry&quot;=c(0.75,0.75,0.74,0.70,0.78,0.68,0.86,0.71),&quot;Fairly_angry&quot;=c(0.13,0.14,0.16,0.17,0.11,0.18,0.06,0.19),&quot;Very_angry&quot;=c(0.02,0.02,0.02,0.05,0.02,0.02,0.03,0.01),&quot;Region&quot;=c(&quot;North&quot;,&quot;Midlands&quot;,&quot;East&quot;,&quot;London&quot;,&quot;South&quot;,&quot;Wales&quot;,&quot;Scotland&quot;,&quot;Northern_Ireland&quot;),&quot;England&quot; = c(&quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;England&quot;, &quot;Not England&quot;, &quot;Not England&quot;, &quot;Not England&quot;)) # make stacked bar chart likert(Region ~.|England, layout=c(1,2), data, positive.order = TRUE, scales=list(y=list(relation=&quot;free&quot;)), strip.left=strip.custom(bg=&quot;gray97&quot;), strip=FALSE, as.percent = &quot;noRightAxis&quot;, ReferenceZero = 2.5, main = &#39;Angry levels in different regions&#39;, ylab = &quot;Region&quot;, xlab = &quot;Percentage&quot;, sub= list(&quot;Angry Level Rating&quot;,x=unit(.6, &quot;npc&quot;))) For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/v6iuiikyxq/YG-X-MarksTheSpot-BrandsAnger-280812.pdf. 6.3 Simple examples Too complicated! Let’s see some simpler examples first. For the below examples, we will use the survey of the satisfaction of Trump for male and female. data_2 &lt;- data.frame(&quot;Great&quot;=c(4,2),&quot;Good&quot;=c(14,6),&quot;Average&quot;=c(15,16),&quot;Poor&quot;=c(17,17),&quot;Terrible&quot;=c(44,48),&quot;Gender&quot;=c(&quot;Male&quot;,&quot;Female&quot;)) data_2 &lt;- data_2[,c(6,1,2,3,4,5)] data_2 ## Gender Great Good Average Poor Terrible ## 1 Male 4 14 15 17 44 ## 2 Female 2 6 16 17 48 For more info on this dataset, go to https://d25d2506sfb94s.cloudfront.net/cumulus_uploads/document/psse08hgpj/YouGov%20-%20Trump%20state%20visit%20190520.pdf. 6.3.1 Stacked bar chart To create a stacked bar chart, we will simply use likert function. likert(Gender ~ ., data_2, ReferenceZero = 0, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to compare the end values using stacked bar chart, but it is hard to compare the neutral percentage. 6.3.2 Diverging stacked bar chart To create a diverging stacked bar chart, we will modify ReferenceZero to the neutral category. likert(Gender ~ ., data_2, ReferenceZero = 3, as.percent = &quot;noRightAxis&quot;, main = &quot;Satisfaction of Trump&quot;) It is easy to visualize the overall shape of likes and dislikes, and the percentage of neutrals. However, it is hard to compare the value of like and dislike categories. 6.4 Stacked bar chart using ggplot In fact, we can make likert plot using ggplot. There is a easy way through dividing the data into two parts which represent agreement and disagreement respectively. Then we can generate the two plots on same place with one of the value of plot is negative. If there is neutral option in data, we have to divide it into two parts, which will be a little more complicated. library(ggplot2) ## ## Attaching package: &#39;ggplot2&#39; ## The following object is masked from &#39;package:latticeExtra&#39;: ## ## layer library(reshape2) library(RColorBrewer) library(dplyr) library(ggthemes) library(stringr) library(forcats) d &lt;- data_2 d[,2:6] &lt;- d[,2:6]/rowSums(d[,2:6]) mytitle &lt;- &quot;Satisfaction of Trump&quot; mylevels &lt;- c(&quot;Great&quot;, &quot;Good&quot;, &quot;Average&quot;, &quot;Poor&quot;, &quot;Terrible&quot;) # Generate mid value of neutral category numlevels &lt;- length(d[1,])-1 numcenter &lt;- ceiling(numlevels/2) + 1 d$midvalues &lt;- d[,numcenter]/2 d_2&lt;-cbind(d[,1],d[,2:ceiling(numlevels/2)], d$midvalues, d$midvalues,d[,numcenter:numlevels+1]) colnames(d_2)&lt;-c(&quot;Sex&quot;,mylevels[1:floor(numlevels/2)],&quot;Midlow&quot;, &quot;Midhigh&quot;,mylevels[numcenter:numlevels]) # Split into six categories numlevels&lt;-length(mylevels)+1 point1&lt;-2 point2&lt;-((numlevels)/2)+1 point3&lt;-point2+1 point4&lt;-numlevels+1 # Assign color to each categories numlevels&lt;-length(d[1,])-1 temp.rows&lt;-length(d_2[,1]) pal&lt;-brewer.pal((numlevels-1),&quot;RdBu&quot;) pal[ceiling(numlevels/2)]&lt;-&quot;#DFDFDF&quot; legend.pal&lt;-pal pal&lt;-c(pal[1:(ceiling(numlevels/2)-1)], pal[ceiling(numlevels/2)], pal[ceiling(numlevels/2)], pal[(ceiling(numlevels/2)+1):(numlevels-1)]) # Generate new data frame including all information d_3&lt;-melt(d_2,id=&quot;Sex&quot;) d_3$col&lt;-rep(pal,each=temp.rows) d_3$value&lt;-d_3$value*100 d_3$Sex&lt;-str_wrap(d_3$Sex, width = 40) d_3$Sex&lt;-factor(d_3$Sex, levels = d_2$Sex[order(-(d_2[,5]+d_2[,6]+d_2[,7]))]) highs&lt;-na.omit(d_3[(length(d_3[,1])/2)+1:length(d_3[,1]),]) lows&lt;-na.omit(d_3[1:(length(d_3[,1])/2),]) # Plot ggplot() + geom_bar(data=highs, aes(x = Sex, y=value, fill=col), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_bar(data=lows, aes(x = Sex, y=-value, fill=fct_inorder(col)), position=&quot;stack&quot;, stat=&quot;identity&quot;, width = 0.5) + geom_hline(yintercept = 0, color =c(&quot;white&quot;)) + scale_fill_identity(&quot;Percent&quot;, labels = mylevels, breaks=legend.pal, guide=&quot;legend&quot;) + theme_fivethirtyeight() + coord_flip() + labs(title=mytitle, y=&quot;&quot;,x=&quot;&quot;) + theme(plot.title = element_text(size=14, hjust=0.5)) + theme(axis.text.y = element_text(hjust=0)) + theme(legend.position = &quot;bottom&quot;) 6.5 Theory Likert data is a type of rating scale commonly used in surveys. It’s a bipolar data, representing the attitude to a statement. We can add neutral options to help candidate make choices when they are uncertain. A typical Likert scale may look like Strongly disagree Disagree Agree Strongly agree or Strongly disagree Disagree Neither agree or disagree Agree Strongly agree The options of likert scale can avoid the distortion of the survey result which is likely to be too extreme. 6.6 When to use When the data is from survey and is likert scale. 6.7 External resources 4 ways to visualize Likert Scales: The advantages and disadvantages of different stacked bar chart. Likert Plots in R: Using ggplot to plot diverging stacked bar chart. "],
["visualization-in-time-series-analysis.html", "Chapter 7 Visualization in Time Series Analysis 7.1 Initiate a Time series object: 7.2 Plot the data: 7.3 Transformation of nonstationary: 7.4 ACF and PACF for time series 7.5 Full model: Yt = T(Trend) + S(Seasonality) +C(Cycle)", " Chapter 7 Visualization in Time Series Analysis Yihao Li (yl4326) Note: All the pseudo-documentation introduction will only include parameter I used often, it’s not the full parameter set. Basic Settings #In case there&#39;s some new package... #install.packages(c(&quot;tidyverse&quot;, &quot;stats&quot;, &quot;forecast&quot;, &quot;dynlm&quot;, &quot;lubridate&quot;, # &quot;strucchange&quot;,&quot;sarima&quot;)) library(tidyverse) library(stats) # a lot of basic operations here library(forecast) # fantastic package with a ton of shortcut library(dynlm) # if you need some linear regression on lag of data library(lubridate) # I didn’t include any function here but necessary for time series library(strucchange) # parameter stability test #library(zoo) #used by forecast, will be automatically included require(&quot;PolynomF&quot;) library(sarima) # not for the analysis, just for simulation in showing 7.1 Initiate a Time series object: stats::ts(data = NA, start = 1, frequency = 1) where: data is the data input start refers to the time of the first observation. Frequency the number of observations per unit of time. 7.2 Plot the data: plot(time, value) Traditional built-in plot, or, ggplot2::autoplot(object) powerful plotting function not only for time series analysis but also for model decomposition object, model fit object, even forecast models. library(expsmooth) # use a dataset cangas here ts_data = cangas autoplot(ts_data) + ylab(&quot;Canadaian Gas Production&quot;) 7.3 Transformation of nonstationary: 7.3.1 Stationarity: Required by most of the time series analysis tools: 1. First Order Weakly Stationary: all R.V.s have the same means. 2. Second Order Weakly Stationary (Covariance Stationary) a. Necessary for most of the time series analysis. b. All R.V.s have same variables, same variances. c. The correlation of time variable only depend on the time difference, i.e. \\(\\rho(Y_t, Y_{t-k}) = \\rho(|k|)\\). 7.3.2 Operations From the plot we have a intuition of its stationarity and we can do following: 1. First Order Weakly Stationary: Take the first difference \\((Y_t – Y_{t-1})\\) of the data. 2. second order weakly stationary: Take the log and then take the first difference(i.e. \\((\\log(y_T) – \\log(y_{T-1}))\\)of this transformed series. 3. Growth rate. ts_data_diff = diff(ts_data) #first-order difference ts_data_log_diff = diff(log(ts_data)) #first-order log difference ts_data_growth_rate = diff(ts_data)/ stats::lag(ts_data, k=-1) #growth rate 7.4 ACF and PACF for time series Autocovariance Function(ACF): \\(corr(y_t ,y_{t-k})\\) Partial Autocorrleation Function(PACF): the autocorrelataion between \\(Y_t\\) and \\(Y_{t+k}\\) after removing(conditioning on) all the observation in between. To save your code line, there’s forecast::tsdisplay(ts_data) which plots the ggplot2::autoplot(), stats::acf(), and stats::pacf() together. It also have a very friendly forecast::ggtsdisplay(ts_data) which returns a ggplot object for you, however, ggtitle() behaves weird in this graph, not to use ggtitle with it. White Noise: Time series process with zero mean, constant variance, and no serial correlation. Most of time use Gaussian y_t ~N(0, \\(\\sigma\\)^2). For pure white noise, both ACF and PACF should be 0, only k = 0 will have ACF = PACF = 1. stats::acf(ts_data_diff) stats::pacf(ts_data_diff) tsdisplay(ts_data_diff) ggtsdisplay(ts_data_diff) 7.5 Full model: Yt = T(Trend) + S(Seasonality) +C(Cycle) Time is the money my friend, you don’t need to waste time to guess the seasonality pattern. 1. stats::decompose(ts_data, type = c(“additive”, “multiplicative”)) Decompose a time series into seasonal, trend and irregular components using moving average. Where type stands for additive/multiplicative seasonal cpomponent. 2. stats::stl(ts_data, s.window) Decompose a time series into seasonal, trend and irregular components using loess. 3. forecast::seasadj(object) Takes in a decompose or stl object. Returns seasonally adjusted data. constructed by removing the seasonal component. 4. You can also find the decomposition data in model. for decomposed.ts object, it’s in x()(original) seasonal, trend, random(remainder). for stl object it’s in time.series(seasonal, trend, and remainder). fit &lt;- decompose(ts_data, type=&#39;additive&#39;) # I use original data here! autoplot(fit) fit %&gt;% seasadj() %&gt;% autoplot() + ggtitle(&quot;Seasonally adjusted data&quot;) fit$trend %&gt;% autoplot() + ggtitle(&quot;Trend&quot;) 7.5.1 Trend(T): Linear, Quadratic, etc. For normal linear model stats::lm(formula, data, na.action) For normal linear regression; dynlm::dynlm(formula, data, na.action) For dynamic linear regression; na.action is optional but I think it saves life from cleaning data. 7.5.2 Seasonality(S): Direct visualization: decompose and stl. one-line beautiful visualization: forecast::seasonplot(ts_data) and it’s ggplot version forecast::ggseasonplot(ts_data). ggseasonplot(ts_data) 7.5.3 Cycle(C): ARIMA family: often used, powerful model in cycle analysis. Notice the model selection is quite subjective except the ARIMA one. 7.5.3.1 1. MA(q) Moving average: \\(Y_t – μ = \\sum_1^q \\theta_i \\epsilon_{t-i}\\) ACF: significant spikes in first q position PACF: decaying in absolute value ma2.sim&lt;-arima.sim(model=list(ma=c(0.7,0.3)),n=100) tsdisplay(ma2.sim) 7.5.3.2 2. AR(p) Autoregressive: \\(Y_t = c +\\sum_1^p \\phi_i Y_{t-i} +\\epsilon_t\\) ACF: decaying in absolute value PACF: significant spikes in first q position. ar2.sim &lt;- arima.sim(model=list(ar=c(0.5,0.1)), n=500) tsdisplay(ar2.sim) 7.5.3.3 3. Seasonal Model with parameter s: s stands for the period, e.g. 4 for quarterly data, 12 for monthly data, etc. 7.5.3.3.1 a. Seasonal-MA(q) Model with parameter s: \\(Y_t – μ = \\sum_1^q\\theta_{is} \\epsilon_{t-is} + \\epsilon_t\\) ACF: spikes at s, 2s,… , ps PACF: spikes at ns decaying s4ma1.sim &lt;- sim_sarima(n=144, model = list(ma=c(rep(0,3),0.8))) # SMA(1), 4 quarters tsdisplay(s4ma1.sim) 7.5.3.3.2 b. Seasonal-AR(p) Model with parameter s: \\(Y_t = c +\\sum_1^p \\phi_{is} Y_{t-is} +\\epsilon_t\\) ACF: spikes at ns decaying PACF: spikes at s, 2s,… , ps s4ar1.sim &lt;- sim_sarima(n=144, model = list(ar=c(rep(0,3),0.8))) # SAR(1), 4 quarters tsdisplay(s4ar1.sim) 7.5.3.4 4. ARMA(p,q) Autoregressive Moving Average: ARMA(p,q) = AR(p) + MA(q), inherited the characters from both AR and MA Saves parameters: ARMA(1,1) is performing better than AR(3). very subjective in the value selection of p and q… ar2ma2.sim&lt;-arima.sim(model=list(ar=c(0.9,-0.2),ma=c(-0.7,0.1)),n=100) #ARMA(2,2) tsdisplay(ar2ma2.sim) 7.5.3.5 5. ARIMA(p,d,q) Autoregressive integrated moving average: stationary and invertible ARMA(p,q) after differencing d times. a. Deal with nonstationary series b. Include d order difference, to remove d or lower order trends. c. Also deal with stochastic trends d. Quantitative one-line data choose forecast::auto.arima(y, ic = c(“aicc”, “aic”, “bic”)) where y is a univariate time series, ic is the criterion. Returns best ARIMA model with seasonsal ARIMA according to either AIC, AICc or BIC value. data = read.table(&quot;resources/Visualization_in_Time_Series_Analysis/w-gs1yr.txt&quot;, header = TRUE) ts_data = ts(data$rate, start = 1962, deltat = 1/52, freq = 52) autoplot(ts_data)+ ggtitle(&quot;US Weekly Interest Rates (%)&quot;)+ ylab(&quot;Interest Rate&quot;) tsdisplay(diff(ts_data)) t = seq(1962, length = length(ts_data), by = 1/52) fit = auto.arima(diff(ts_data)) summary(fit) ## Series: diff(ts_data) ## ARIMA(1,0,2) with zero mean ## ## Coefficients: ## ar1 ma1 ma2 ## 0.6284 -0.3065 -0.0527 ## s.e. 0.0642 0.0675 0.0299 ## ## sigma^2 estimated as 0.03143: log likelihood=768.32 ## AIC=-1528.65 AICc=-1528.63 BIC=-1505.41 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ## Training set -0.0006226569 0.1771898 0.1047296 NaN Inf 0.6423691 ## ACF1 ## Training set -0.0002639336 plot(t[-1], diff(ts_data), main = &quot;US Weekly Interest Rates (%) (first difference)&quot;, ylab = &quot;Rates&quot;, xlab = &quot;Time&quot;, type = &quot;l&quot;) lines(fit$fitted, col = &quot;blue&quot;) 7.5.3.6 Aside: AIC(Akaike Information Criterion) biased towards overparameterized models asymptotically efficient stats::AIC(fit, k=2) where: fit is the a fitted model object, k is the penalty per parameter to be used; the default k = 2 is the classical AIC. BIC/SIC(Bayesian (Schwarz) Information Criterion) Consistent not asymptotically efficient stats::BIC(fit) where fit is the a fitted model object Both AIC and BIC are the smaller the better. 7.5.4 Summary 7.5.4.1 Steps The full model includes a trend, seasonal dummies, and cyclical dynamics For a given visualized data: a. Extract the trend and seasonality (stl) b. ACF &amp; PACF, Remove any possible dynamics(tsdisplay, auto.arima) c. Check if the residuals looks like a white noise 7.5.4.2 Residuals vs White Noise #this fit is following the last chunk tsdisplay(resid(fit)) forecast::checkresiduals(fit) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(1,0,2) with zero mean ## Q* = 401.53, df = 101, p-value &lt; 2.2e-16 ## ## Model df: 3. Total lags used: 104 Note: 1. Object in resid() can be either a time series model, a forecast object, or a time series (assumed to be residuals). 2. In the new graph, the scale of ACF and PACF is significantly smaller, thus we have removed some of the dynamics. 7.5.4.3 Test Parameter stability CUSUM(Cumulative Sum of Standardized Recursive Residuals): Check if the CUSUM leave the confidence interval to capture the dynamics. Recursive Residuals: strucchange::recresid(x, y, formula) autoplot(ts(recresid(fit$res ~ t[-1])))+ ggtitle(&quot;The recursive residual of ARIMA(1,0,2)&quot;)+ ylab(&quot;recursive residual&quot;) Empirical Fluctuation Processes: efp(x, y, formula, type = “Rec-CUSUM”) plot(efp(fit$res ~ t[-1], type = &quot;Rec-CUSUM&quot;)) Note: 1. In the recursive residual’s visialization, we find some instability in the middle. 2. Empirical Fluctuation Processes showed the parameter’s stability is acceptable. 7.5.5 Reference: www.rdocumentation.org My sincere appreciation to Dr. Randall Rojas (UCLA Econ 144 19’ Spring)’s work. The general model’s idea, both data set: cangas and the tidied “w-gs1yr.txt” is from the Econ 144 class. "],
["shiny.html", "Chapter 8 Shiny 8.1 Part 1 How to Build a Shiny App 8.2 1. Install the shiny package 8.3 2. Template for creating a shiny app 8.4 3. Add elements to user interface using fluidPage() 8.5 4. Build output in server instructions 8.6 5. Share your app 8.7 Part 2 How to Customize Reactions 8.8 1. Reactivity 8.9 3. Summary", " Chapter 8 Shiny Duanyue Yun, Boyu Liu In this tutorial, we will use the cars dataset as an example to wall through the process of building a shiny app. The cars dataset contains various information about a particular car. cars_info &lt;- read.csv(&quot;cars.csv&quot;) 8.1 Part 1 How to Build a Shiny App 8.2 1. Install the shiny package First of all, we can install the shiny package by running the code below. install.packages(&quot;shiny&quot;) 8.3 2. Template for creating a shiny app A shiny app consists of two main components: user interface (ui) and server instructions (server). The user interface will contain the elements that a user sees on your shiny app, which can be input (possible user interactions) and output display. The server instructions will define how the app should react to a user’s action. Therefore, a basic template for creating a shiny app consists of 3 parts as shown below: library(shiny) #1 define user interface ui &lt;- fluidPage() #2 define server instructions server &lt;- function(input, output) {} #3 putting everything together shinyApp(ui = ui, server = server) 8.4 3. Add elements to user interface using fluidPage() The arguments of the fluidPage() function could be Input() functions or Output() functions. 8.4.1 Input functions Inputs define the possible ways a user can interact with our shiny App. For a numerical variable, the input could be a slider that a user can move along to select a certain value. For a categorical variable, the input could be a box where the user can select a particular category from a drop down list. All Input() functions contain 2 required arguments: inputId = and label =. inputId is for us to identify a particular input. Later we can use the same input ID in server instructions to decide the corresponding output. Therefore, to avoid errors, it is better to give a unique name to each input. label is what the user sees on the shiny App, so it should be informative. The common Input() functions supported are: actionButton(), submitButton(), checkboxInput(), checkboxGroupInput(), dateInput(), dateRangeInput(), fileInput(), numericInput(), passwordInput(), radioButtons(), selectInput(), sliderInput(), textInput(). Each Input() function has some specific arguments. For example, the sliderInput() function requires min, max arguments to set the range of the slider and also a value argument which is the default value the user sees when the shiny app is launched. You can find more about the function using ?sliderInput(). For example, we can add a select box by running the code below. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)])) 8.4.2 Output functions We can display an output, for example a plot, by adding Output() functions to fluidPage(). Each Output() function requires one argument, which is outputId =. We will talk about how to build output in server instructions. ui &lt;- fluidPage(plotOutput(&quot;histogram&quot;)) The common Output() functions supported are: dataTableOutput(), htmlOutput(), imageOutput(), plotOutput(), tableOutput(), textOutput(), uiOutput(), verbatimTextOutput(). 8.5 4. Build output in server instructions 8.5.1 (1): Save objects you want to display to output$ server &lt;- function(input, output) { output$histogram &lt;- # code } We can use the same name in the form of a string in fluidPage() to display the output. 8.5.2 (2): Build objects with render() The render() functions that are supported include renderDataTable(), renderImage(), renderPlot(), renderPrint(), renderTable(), renderText(), renderUI(). Within render() functions, we could use {} to wrap the code so that we can write multiple lines of code to create more sophisticated output. As an example, the following code builds a histrogram of the variable mpg to our shiny app. Remember to add it to ui() to display it in the shiny app. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info$mpg, main = &quot;&quot;, xlab = &quot;mpg&quot;) }) } 8.5.3 (3): Use input values with input$ When we use input$, the app will be interactive. For the following example, when the user selects a different variable, the histogram will change accordingly. server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } 8.6 5. Share your app 8.6.1 Save your app You should save your app to one directory with every file the app needs: app.R (must be the exact file name) datasets, images, css, helper scripts, etc. 8.6.2 Publish your app on Shinyapps.io Go to https://www.shinyapps.io to sign up for an account. When you signed up for a new account on https://www.shinyapps.io, there will be instructions on how to associate your account with your RStudio IDE and how to deploy your app. 8.7 Part 2 How to Customize Reactions 8.8 1. Reactivity 8.8.1 What is reactivity? Let’s think about Microsoft Excel. In Excel, we can type some value into a cell x and type a formula that uses x into a new cell y. Then whenever we change the value in x, y’s value will change correspondingly. This is reactivity, which is also what Shiny does. Shiny has an input object input$x and an output object output$y. Any changes in input$x will cause changes in output$y. So now let’s start with reactive values, which is where reactivity starts in Shiny. 8.8.2 Reactive values Reactive values are what the user selects and depend on Input() functions. In the previous example where we create a select box, the reactive values are the variable that the user selects. Note that reactive values don’t work on their own. They actually work together with reactive functions. 8.8.3 Reactive functions (reactive toolkit) They are a kind of functions that are expected to take reactive values and know what to do with them. They are notified that they need to re-execute whenever the reactive value changes. They are included in the server instructions section to build (and rebuild) an object. We can think of reactivity in R as two-step process. Consider the following example. We use input function selectInput() to get user’s choice. input$varname is the reactive value. When we choose different variables, firstly reactive values will notify the functions which use them that they become outdated. After that its job is over and it’s time for reactive functions to do their jobs, which is rebuild the corresponding object using new values. The process is automatic in shiny. Suppose we want to output the corresponding histogram whenever the user chooses a variable. ui &lt;- fluidPage( # Add a select box selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), # Add corresponding output plotOutput(outputId = &quot;histogram&quot;) ) server &lt;- function(input, output) { output$histogram &lt;- renderPlot({ hist(cars_info[[input$varname]], main = &quot;&quot;, xlab = input$varname) }) } shinyApp(ui = ui, server = server) 8.8.4 Modularize code with reactive() See the example below. When the user selects a number, the shiny app plots a histogram for that number of N(0,1) variables and also computes the summary statistics. This app has only one reactive value (number the user chooses) but has two objects, a histogram and a block of text which includes the statistics of the data. When the reactive value changes, it will notify these two objects and they will rerun the code to update themself. But the probelm is that because they rerun their code successively, so rnorm(input$num) is called twice. Since rnorm is random, each object generates a different set of values, which means the histogram describes a dataset and the summary of the statistics describes another dataset. # Before ui &lt;- fluidPage( # Add a slider sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), # Display the histogram plotOutput(outputId = &quot;hist&quot;), # Display the summary statistics verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { # Build the histogram output$hist &lt;- renderPlot({hist(rnorm(input$num), main = &quot;&quot;, xlab = &quot;num&quot;)}) # Build the object that contains the summary statistics output$stats &lt;- renderPrint({summary(rnorm(input$num))}) } shinyApp(ui = ui, server = server) For example, when we only select one normal variable. It is clear that the histogram and the summary statistics do not correpond to the same data. Can the two objects describe the same data? The answer is yes! The strategy is calling rnorm(input$num) only once and saving the dataset it creates. Then use this dataset downstream when we need it. Shiny provides a function called reactive(), which can wrap a normal expression to create a reactive expression and realize what we hope to achieve. reactive(rnorm(input$num)) In this specific example, we add a code data &lt;- reactive(rnorm(input$num)) to the server and replace rnorm(input$num) in the reactive functions with data(). Note that you should call a reactive expression like a function. So here we use data() instead of data. # After ui &lt;- fluidPage( sliderInput(inputId = &quot;num&quot;, label = &quot;Please choose a number.&quot;, min = 1, max = 100, value = 25), plotOutput(outputId = &quot;hist&quot;), verbatimTextOutput(&quot;stats&quot;) ) server &lt;- function(input, output) { data &lt;- reactive(rnorm(input$num)) output$hist &lt;- renderPlot({hist(data(), main = &quot;&quot;, xlab = &quot;num&quot;)}) output$stats &lt;- renderPrint({summary(data())}) } shinyApp(ui = ui, server = server) Now when will select 1 variable, the two objects will describe the same data. 8.8.5 Prevent reactions with isolate() Sometimes we might want to delay a reactive function. For example, the following shiny app plots a scatterplot between 2 variables of the user’s choose and also allows the user to give the plot a customized title. So there are 3 inputs: the title of the scatterplot, an x variable and a y variable. With our regular code, the title will change instantaneously as the user types. # Before ui &lt;- fluidPage( textInput(inputId = &quot;title&quot;, label = &quot;Enter a title&quot;, value = &quot;displacement vs mpg&quot;), selectInput(&#39;xcol&#39;, &#39;X Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)]), selectInput(&#39;ycol&#39;, &#39;Y Variable&#39;, colnames(cars_info)[c(2, 4, 5, 6, 7)], selected=colnames(cars_info)[[4]]), plotOutput(&#39;scatterplot&#39;) ) server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], main = input$title) }) } shinyApp(ui = ui, server = server) Say we do not want the title to change until the user has chosen two variables. In this case, we can use isolate() to isolate the input title. It returns the result as a non-reactive value. That means the observed object will only react to its changes when other inputs also change. server &lt;- function(input, output) { output$scatterplot &lt;- renderPlot({ plot(cars_info[, c(input$xcol, input$ycol)], # This line isolates the input title main = isolate({input$title})) }) } 8.8.6 Trigger code with observeEvent() we can create an action button or link whose value is initially zero, and increments by one each time it is pressed. When we have an input like action button, we can trigger a response when the user clicks on the button by using observeEvent() function. Examples of an action button include download which allows the user to download a file. actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) The observeEvent() function takes two arguments: the first argument is the reactive value(s) it responds to. In our example, it will be the action button. The second armgument is a code block which runs behind the scene whenever the input changes. observeEvent(input$download, {print(input$download)}) Here is how we can use it in our app. Every time we click the Go! button, the observer will update, which is running the block of code print(as.numeric(input$goButton). The result won’t appear in the user panel, but to appear back of our app. ui &lt;- fluidPage( actionButton(inputId = &quot;download&quot;, label = &quot;Download&quot;) ) server &lt;- function(input, output) { observeEvent(input$downloadn, { print(as.numeric(input$download)) }) } shinyApp(ui = ui, server = server) Along with observeEvent() which triggers code, there’s another function called observe(), which does the same thing and it’s a parallel of observeEvent(). But its syntax is more like render*() functions. We just give a block of code to it and it will respond to every reactive value in the code. observe({print(input$download)}) 8.8.7 Delay reactions with eventReactive() Sometimes we don’t want the outputs to change as soon as the user changes some input in the user interface. Instead, we would like to change them when the user clicks an ‘update’ button. In others words, we hope to prevent the output from updating until the user hits the button. The way to do this in Shiny is with the function eventReactive(). It creates a reactive expression that only responds to specific values, similar to reactive() but having different syntax. First we give a reactive value to it. The second argument is the code the function uses to build or rebuild the object when it’s clicked. In addition, similar to observeEvent(), the expression treats this block of code as if it has been isolated with isolate(). data &lt;- eventReactive(input$update, {rnorm(input$num)}) Let’s look at the entire code. ui &lt;- fluidPage( selectInput(inputId = &quot;varname&quot;, label = &quot;Choose a variable&quot;, choices = colnames(cars_info)[c(2, 6, 7)]), actionButton(inputId = &quot;update&quot;, label = &quot;Update&quot;), plotOutput(outputId = &quot;hist&quot;) ) server &lt;- function(input, output) { data &lt;- eventReactive(input$update, {input$varname}) output$hist &lt;- renderPlot({ hist(cars_info[[data()]], main = &quot;&quot;, xlab = data()) }) } shinyApp(ui = ui, server = server) If we choose different variable without clicking Update button, the histogram would not be updated. 8.8.8 Manage state with reactiveValues() We know that the reactive value changes whenever a user changes the input in the user panel. But we cannot set these values in our code. Fortunately, although Shiny doesn’t give us the power to overwrite the input values in our app, it gives us the power to create our own list of reactive values, which you can overwrite. reactiveValues() is a function that creates a list of reactive values to manipulate programmatically. Note that it has nothing to do with input reactive values. rv &lt;- reactiveValues(data = rnorm(100)) Let’s look at an example. If we click mpg vs displacement, the Shiny app would select column mpg and displacement from the cars_info dataset and plot a scatter plot for them. If we click mpg vs weight, it would select column mpg and weight from the cars_info dataset and plot a scatter plot for them. ui &lt;- fluidPage( actionButton(inputId = &quot;scatter1&quot;, label = &quot;mpg vs displacement&quot;), actionButton(inputId = &quot;scatter2&quot;, label = &quot;mpg vs weight&quot;), plotOutput(&quot;scatter&quot;) ) server &lt;- function(input, output) { rv1 &lt;- reactiveValues(data = cars_info[,2], label = &quot;mpg&quot;) rv2 &lt;- reactiveValues(data = cars_info[,4], label = &quot;displacement&quot;) observeEvent(input$scatter1, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,4] rv2$label &lt;- &quot;displacement&quot; }) observeEvent(input$scatter2, { rv1$data &lt;- cars_info[,2] rv1$label &lt;- &quot;mpg&quot; rv2$data &lt;- cars_info[,6] rv2$label &lt;- &quot;weight&quot; }) output$scatter &lt;- renderPlot({ plot(rv1$data, rv2$data, xlab = rv1$label, ylab = rv2$label)}) } shinyApp(ui = ui, server = server) 8.9 3. Summary Till now, We have learnt both syntax and usage of the basic reactive functions in Shiny. Now there are still some important tips we need to provide. We should reduce repetition when we create shiny apps. That is to place code where it will be re-run as little as necessary. Keep in mind that, Code outside the server function will be run once per R session (worker). So you only need it to run once when setting up the R session, outside the server function. For example, codes that load the help file or some library should be placed outside the server function. Code inside the server function will be run once per end user session (connection). Code inside the reactive function will be run once per reaction, which means many times. If you are interested in Shiny and would like to learn more about it, you can go to the official website or download the documentation of Shiny. The relevant resources are listed below. Official website: https://shiny.rstudio.com/ Documantation of pacakge “Shiny”: https://cran.r-project.org/web/packages/shiny/shiny.pdf Share your Shiny apps: https://www.shinyapps.io/ Shiny cheat sheet: https://shiny.rstudio.com/images/shiny-cheatsheet.pdf The main source of this tutorial is the video on the Shiny official website. We adapted it with some new examples based on the cars dataset. Hope this can help you and any suggestion is welcome. "],
["ice-cream-survey.html", "Chapter 9 Ice Cream Survey 9.1 Overview 9.2 Loading packages and reading in data 9.3 Understanding what cleaning is required 9.4 Cleaning and prepping the data 9.5 Visualizing the data 9.6 Takeaways", " Chapter 9 Ice Cream Survey Jake Stamell 9.1 Overview 9.1.1 Description For my community contribution, I sent out a short (4 question) survey to the class on their ice cream preferences. I asked age, country of origin, cup or cone, and favorite flavor. Country and flavor were open text responses, which I hoped would cause some variation in input requiring cleaning (misspellings, alternative flavor names, etc.). 9.1.2 Goals of this community contribution Share the ice cream preferences of our class! Demonstrate how to clean messy text responses in order to ease analysis and visualization Provide an example of visualizing multiple categorical variables 9.2 Loading packages and reading in data library(googlesheets) # For accessing the responses library(tidyverse) # For data cleaning and visualizing library(data.table) # Alternative to dplyr library(countrycode) # For handling country names library(gridExtra) # For visualizations # This key should allow anyone to access the raw results ice_cream_key &lt;- &quot;18uYxylZazzedLVzo4hSLEemQNKZ0uNse_V2aVyARvj8&quot; ice_cream &lt;- gs_key(ice_cream_key) ## Worksheets feed constructed with public visibility ice_cream_responses &lt;- ice_cream %&gt;% gs_read(ws=&quot;Form Responses 1&quot;) ## Accessing worksheet titled &#39;Form Responses 1&#39;. ## Parsed with column specification: ## cols( ## Timestamp = col_character(), ## `What is your age?` = col_double(), ## `What country are you from?` = col_character(), ## `Do you prefer a cup or cone?` = col_character(), ## `What is your favorite flavor of ice cream?` = col_character() ## ) setDT(ice_cream_responses) # using data.table instead of dplyr names(ice_cream_responses) &lt;- c(&quot;Timestamp&quot;,&quot;Age&quot;,&quot;Country&quot;,&quot;Method&quot;,&quot;Flavor&quot;) 9.3 Understanding what cleaning is required For country, respondents were allowed to input whatever they wanted. This caused issues with USA and Mexico, where the former was submitted in multiple formats and the latter included the name with and without accents. A bigger issue is the number of countries with only one respondent, indicating that we will need to combine responses in some way. ice_cream_responses[,.N,by=Country][order(-N)] ## Country N ## 1: India 9 ## 2: China 9 ## 3: USA 6 ## 4: Belgium 1 ## 5: Korea 1 ## 6: France 1 ## 7: Thailand 1 ## 8: Japan 1 ## 9: Indonesia 1 ## 10: México 1 ## 11: Bangladesh 1 ## 12: Usa 1 ## 13: Mexico 1 ## 14: United States 1 For flavor, again respondents could input any text. As expected, this caused many variations on the same flavors and even one misspelling. Even after accounting for this, the same problem remains of having many categories with few responses. The approach I will take for summarizing this will be to group similar flavors (e.g. chocolate chip and chocolate chip cookie dough). I consider myself somewhat of an ice cream expert (making it is a hobby of mine); I will leverage this knowledge in grouping ice cream flavors. ice_cream_responses[,.N,by=Flavor][order(-N)] ## Flavor N ## 1: Chocolate 11 ## 2: Vanilla 6 ## 3: Strawberry 2 ## 4: Chocolate chip cookie dough 1 ## 5: Mint Chocolate Chip 1 ## 6: vanilla 1 ## 7: chocalate 1 ## 8: Black Sesame 1 ## 9: Mint chocolate 1 ## 10: Vanilla macadamia 1 ## 11: Butter pecan 1 ## 12: Vanilla Chocolate Chip 1 ## 13: matcha 1 ## 14: Strawberry Cheesecake 1 ## 15: Chocolate Chip 1 ## 16: Mint 1 ## 17: Coffee 1 ## 18: Chocolate with nuts 1 ## 19: Mint Chocolate 1 Lastly, there are no issues with the data entry for age; however, we need to group it in some way as well. ice_cream_responses[order(Age),.N,by=Age] ## Age N ## 1: 21 2 ## 2: 22 16 ## 3: 23 3 ## 4: 24 3 ## 5: 25 5 ## 6: 26 2 ## 7: 28 1 ## 8: 33 1 ## 9: 34 1 ## 10: 38 1 9.4 Cleaning and prepping the data 9.4.1 Country We start by removing the accent in Mexico. Then, we can take care of two issues at once: duplicate country names and too many countries with small number of responses. By leveraging the countrycode package, we can group countries by continent. This leaves a small number of categories, which will ease our visualizations. Unfortunately, I did not have too many European respondents for this survey so I will create a secondary continent variable that groups them with the Americas. ice_cream_responses[,country_encoding := Encoding(Country)] ice_cream_responses[country_encoding==&quot;UTF-8&quot;, Country := iconv(Country,from=&quot;UTF-8&quot;,to=&quot;ASCII//TRANSLIT&quot;)] ice_cream_responses[,country_encoding := NULL] ice_cream_responses[,Continent := fct_infreq(countrycode(sourcevar= Country, origin= &quot;country.name&quot;, destination= &quot;continent&quot;))] ice_cream_responses[Continent==&quot;Asia&quot;, Continent2 := &quot;Asia&quot;] ice_cream_responses[Continent!=&quot;Asia&quot;, Continent2 := &quot;Americas\\nand Europe&quot;] ice_cream_responses[,Continent2 := fct_infreq(Continent2)] ice_cream_responses[,.N,by=Continent][order(-N)] ## Continent N ## 1: Asia 23 ## 2: Americas 10 ## 3: Europe 2 9.4.2 Flavor For flavor, we start by converting everything to lower case and making sure there is no extra whitespace. The approach for grouping flavors is to use regex to find specific strings and rename the flavors accordingly. Note the order used in “flavors_to_identify”. Some of these will be matched multiple times (e.g. choc and chocolate chip will match chocolate chip cookie dough). Therefore, I have ordered it so that the last match is the one I want to assign and will be the one used. Also, note the use of “choc” to deal with the mispelling of “chocolate” in one response. (This is a little bit of a quick workaround so as to not have to explicitly deal with the issue.) I again create a secondary flavor variable with only 3 categories to compare chocolate-based ice cream against vanilla-based. ice_cream_responses[,Flavor := str_squish(str_to_lower(Flavor))] flavors_to_identify &lt;- c(&#39;strawberry&#39;, &#39;choc&#39;, &#39;vanilla&#39;, &#39;chocolate chip&#39;, &#39;mint&#39;) flavor_names &lt;- c(&#39;Strawberry&#39;, &#39;Chocolate&#39;, &#39;Vanilla&#39;, &#39;Choc chip&#39;, &#39;Mint&#39;) flavor_names2 &lt;- c(&#39;Other&#39;, &#39;Chocolate&#39;, &#39;Vanilla&#39;, &#39;Vanilla&#39;, &#39;Other&#39;) for(i in seq(flavors_to_identify)){ ice_cream_responses[str_which(Flavor, flavors_to_identify[i]), `:=` (Flavor_group=flavor_names[i], Flavor_group2=flavor_names2[i])] } ice_cream_responses[is.na(Flavor_group), `:=` (Flavor_group=&quot;Other&quot;, Flavor_group2=&quot;Other&quot;)] ice_cream_responses[, `:=`(Flavor_group= fct_relevel(fct_infreq(Flavor_group),&quot;Other&quot;,after=Inf), Flavor_group2= fct_relevel(fct_infreq(Flavor_group2),&quot;Other&quot;,after=Inf))] ice_cream_responses[,.N,by=Flavor_group][order(-N)] ## Flavor_group N ## 1: Chocolate 13 ## 2: Vanilla 8 ## 3: Mint 4 ## 4: Other 4 ## 5: Choc chip 3 ## 6: Strawberry 3 9.4.3 Age This is quick to clean as we can just split into 2 groups (basically recent grads and those who have some work experience). ice_cream_responses[Age &lt; 23, Age_group := &quot;&lt;23&quot;] ice_cream_responses[Age &gt;= 23, Age_group := &quot;23+&quot;] ice_cream_responses[,Age_group := factor(Age_group, levels = c(&quot;&lt;23&quot;,&quot;23+&quot;))] 9.5 Visualizing the data 9.5.1 Getting an overview We start with a basic plot to understand the distribution of the transformed variables. Now using the simplified categories. 9.5.2 Ice cream preferences by continent and age Unforunately, even with the simplified bucketing of the variables, splitting the data by all 4 variables of interest reduces each category to a very small count. ice_cream_responses[,.N,by=.(Age_group,Continent2,Method,Flavor_group2)] ## Age_group Continent2 Method Flavor_group2 N ## 1: 23+ Americas\\nand Europe Cup Vanilla 3 ## 2: 23+ Asia Cup Other 2 ## 3: &lt;23 Asia Cup Vanilla 2 ## 4: &lt;23 Asia Cup Chocolate 2 ## 5: 23+ Asia Cone Other 4 ## 6: &lt;23 Asia Cone Chocolate 4 ## 7: 23+ Americas\\nand Europe Cone Vanilla 2 ## 8: &lt;23 Asia Cup Other 1 ## 9: &lt;23 Americas\\nand Europe Cone Chocolate 2 ## 10: &lt;23 Asia Cone Vanilla 2 ## 11: &lt;23 Americas\\nand Europe Cone Other 2 ## 12: 23+ Asia Cone Vanilla 1 ## 13: &lt;23 Asia Cone Other 2 ## 14: 23+ Asia Cone Chocolate 3 ## 15: &lt;23 Americas\\nand Europe Cone Vanilla 1 ## 16: 23+ Americas\\nand Europe Cone Chocolate 2 Therefore, we will need to compare 2 or 3 variables a at a time to find the interesting patterns. Even breaking down the responses by continent produces buckets that are a little too small for comparison. Still, it looks like respondents from Asia are more likely to favor “Other” flavor than the classics as compared to respondents from Americas/Europe. Additionally, &lt;23/Asia and 23+/Americas,Europe may like cups more than the others. However, maybe there is something else at play? Turns out people who like vanilla are more likely to also prefer a cup! Let’s dig in to age now Age doesn’t apear to play a major role in preferences. Maybe this isn’t too surprising since I chose an arbitrary split for the two groups! 9.6 Takeaways N = 35 is very small when you have 4 variables of interest with multiple categories each. Analyzing respones required bucketing each variable of interest into 2 or 3 categories. Chocolate and vanilla based ice creams are about equally preferred. Cones are much more popular than cups. Respondents who liked vanilla were more likely to prefer cups. Age did not play a major role in preferences, likely due to a small age range of 17 years. "],
["self-reflection-demographical-discoveries-using-tinder-data.html", "Chapter 10 Self-Reflection &amp; Demographical Discoveries Using Tinder Data 10.1 Introduction 10.2 Analysis 10.3 Conclusion 10.4 Final Thoughts", " Chapter 10 Self-Reflection &amp; Demographical Discoveries Using Tinder Data Benjamin Livingston 10.1 Introduction When I told my friends I was doing this, they laughed. After I showed this to my friends, they laughed again. I laughed, too. Our Tinder data is a disturbingly accurate window into our romantic selves. It traces so many of our dating tendencies, from pickiness, to obsession, to desperation, to pushiness. I gained tremendous insight into my romantic habits from this exercise, and I hope you will enjoy it as much as I did. Most importantly, I’ve constructed this in a way that will allow you to easily do this analysis for yourself, too. 10.1.1 For The Taken / Non-Millennial Folk You’re probably going to look at every statistic and graph here and wonder, “what the heck is all this?” Tinder is a dating app that launched in 2012, available from any web browser or smartphone. You create a profile, select your preferred gender, age, and locational proximity for a potential partner, and Tinder provides you a sequence of other users that fit your criteria. Every time a user’s profile pops up, you can either “swipe left” and pass on them, or “swipe right” and like them. If (and only if) you and the other user both swipe right on each other, you are deemed a “match”, and you gain ability to talk to one another. (via Innovation Is Everywhere) User habits vary: some users swipe right on everyone they see, while some users are very picky. There is very little explicit feedback from the app, so the user is forced to form their own conclusions from their personal data, which Tinder allows you to download. 10.1.2 Replicating This Analysis For Yourself I’ve made it possible for you to create all these statistics and graphs for yourself at the click of a button. Your Tinder data can be downloaded at this link. In this GitHub repository, you will find a file called grabyourtinder.R. If you download your Tinder data as instructed, you will receive a zipped file. In that file, there is a JSON labeled “data.JSON”. This is your Tinder data - namely, all your messages and daily statistics. The code I wrote for this project allows any user to extract all of their daily usage statistics from this JSON without the need for additional software. If you extract and copy data.JSON to your R working directory and run the code in grabyourtinder.R, you will be able to create all these graphs and statistics for yourself, and generate a .csv of your Tinder data. Try this!!!! I did all the legwork for you. I’d love to hear what you come up with. If you don’t like what you see, you can throw your laptop out the window and the evidence will disappear forever. A note for non-R users: If you haven’t learned R, this is the perfect time to. It’s free, extremely easy to use, fun to play with, and very powerful. Two recommended free resources if you’d like to try it out: Hadley Wickham’s R For Data Science Roger D. Peng’s R Programming For Data Science 10.1.3 Protecting The Innocent (and Not-So-Innocent) Since my Tinder data JSON file also contains my message data, it will unfortunately not be made available with this project. As you will see soon, there are a lot of messages in there, and thus a plethora of personally identifiable information (for myself and others) that can’t be posted on the internet. Hope you understand. In lieu of this, I have included a .csv file with my daily usage statistics in the GitHub repository, which was extracted from the JSON using my script. 10.1.4 A Fun Twist I will be plotting my Tinder usage over time, and I’m going to add an extra feature to spice it up. This data covers 2014-2015 until the present (we will explain why the start of this range is indefinite later). In Fall 2016, I moved from Pittsburgh to Philadelphia, and then in Summer 2019, I moved from Philadelphia to New York. We will mark those moves in our graphs, and see if we discover any geographic trends as we conduct our analysis. 10.2 Analysis 10.2.1 Our Fun New Tinder Statistics: “Amourmetrics” Opens - the number of times I opened the Tinder app Messages - messages exchanged on the app (split by sent vs. received where stated, combined otherwise) Likes - the number of times I swiped right (a.k.a. “liked” a user) Passes - the number of times I swiped left (a.k.a. “passed” on a user) Swipes - the total number of times I swiped, equal to likes + passes 10.2.2 All-Time Statistics &amp; A Demographical Discovery Let’s start by examining my messaging habits. print(paste0(&#39;Total messages sent: &#39;,sum(bentinder$messages_sent))) ## [1] &quot;Total messages sent: 23047&quot; print(paste0(&#39;Total messages received: &#39;,sum(bentinder$messages_received))) ## [1] &quot;Total messages received: 19156&quot; print(paste0(&#39;Total messages: &#39;,sum(bentinder$messages_sent)+sum(bentinder$messages_received))) ## [1] &quot;Total messages: 42203&quot; I’m a talkative person, so this isn’t particularly surprising. What’s most interesting about this talk-versus-listen trend is how it has varied over time, which we’ll get to in a bit. Of course, your reaction may be a more primal “FORTY TWO THOUSAND MESSAGES?!?!”. If that’s the case, wait until you see my all-time totals across all Tinder statistics. messages = bentinder %&gt;% select(date,messages_sent,messages_received) %&gt;% mutate(message_differential = messages_received - messages_sent) bentinder = bentinder %&gt;% mutate(messages = messages_sent + messages_received) %&gt;% select(-c(messages_sent,messages_received)) bentinder = bentinder %&gt;% mutate(swipes=likes+passes) sapply(bentinder[-1],sum) ## opens likes passes matches messages swipes ## 25081 75404 214505 8777 42203 289909 289,909 swipes! 289,909! This is all mind-blowing… but 289,909?? This could make you laugh, cry, drop your jaw, or just rub your temples and shake your head. But there’s a deeper meaning to this number that I’d like to explore - because considering that I only date men, it’s completely incomprehensible. Think about this for a moment. According to a 2006 study by UCLA’s Gary J. Gates (the most recent readily-available, exhaustive empirical estimate of metropolitan area LGBT populations), the 2005 LGBT populations of the metropolitan areas I’ve lived in were approximately as follows: Pittsburgh: 50,994 Philadelphia: 179,459 New York: 568,903 Furthermore, the LGBT population of Pennsylvania as a whole was 323,454. While these numbers have likely grown in the last decade-and-a-half, they don’t seem to have skyrocketed significantly based on more recent city LGBT population estimates, nor does it make intuitive sense that the number of gay men would have grown astronomically in the last 15 years. Surely, more people are openly LGBT in 2019 than in 2005, but we are making an important distinction between LGBT and openly LGBT (or identifying as LGBT) here. In other words, telling a survey-taker that you are LGBT and being LGBT (and seeking same-sex partners on Tinder) can be two very different things. A quick calculation finds that 273,682 of these swipes happened before I moved to New York. The combined LGBT adult populations of the Pittsburgh and Philadelphia areas is estimated at 230,453, and the entire state only has an estimated 323,454 LGBT adults. If we make a loose assumption that about half of the LGBT residents are male, that would leave about 115,000 gay men in the Philadelphia and Pittsburgh areas and about 162,000 gay men in Pennsylvania at large. I swiped 273,000 times while I lived in Pennsylvania. That means I swiped more than twice the number of available people in my cities and over 1.5 times the number of gay men in my state. Considering I typically set Tinder to only show me people close to my age (almost exclusively within five years), this doesn’t make any sense. This makes me wonder if these LGBT population estimates are even close to accurate. I swiped a lot while out of town (or while using Tinder’s Passport feature) and with visitors from other places, and while I can’t definitively state exactly how much of my swiping was done with people from other metropolitan areas, it probably isn’t enough to explain this trend. Even if only 50,000 of my swipes were done with people residing in my metropolitan area (which would be less than 20% of my overall swipes), these numbers still don’t add up. Tinder typically doesn’t suggest the same person twice, so we can probably rule that out as a major factor. It seems very likely I saw at least 200,000 unique people, and we will make a low-end estimate that 50,000 of them lived in the Philadelphia and Pittsburgh areas. It seems extremely unlikely that that 50,000 of the estimated 115,000 LGBT adult males in those areas are Tinder users close to my age. These numbers suggest there are (and have probably long been) many more gay men in these cities than the aforementioned research stated in 2006, and other self identification-based research has stated since. The linked Gallup article states that “Estimate of LGBT Population Rises to 4.5%”. My data casts serious doubt on the validity of these estimates. In no way am I claiming definitive proof that these figures are wrong, but even a cursory glance at my numbers makes them seem like poor estimates. There are many potential mitigating factors here that prevent any sort of sound empirical proof of this assertion. However, the most parsimonious, plausible explanation is that the true number of gay men in America (or at the very least, in Pennsylvania) hasn’t been anywhere close to properly enumerated in studies that rely on self-identification. Of course… perhaps all this analysis is simply a ploy on my part to deflect from the fact that I have swiped two hundred and eighty-nine thousand times on Tinder. I can’t wrap my head around that number any more than you can. Let’s continue this dive into insanity by examining my all-time daily maximums. sapply(bentinder[-1],max) ## opens likes passes matches messages swipes ## 172 1632 3548 91 509 5144 I don’t need any advanced statistical data analysis to tell you that opening Tinder 172 times in one day and swiping 5144 times in a day is… well I’ll let you pick a word for it. I’m curious though… what was happening on those days? Let’s check the records and find out. bentinder %&gt;% filter(opens==172|likes==1632|passes==3548|matches==91|messages==509|swipes==5144) %&gt;% mutate(day = wday(date,label = T)) ## date opens likes passes matches messages swipes day ## 1 2016-04-10 135 1632 3512 91 289 5144 Sun ## 2 2016-04-12 91 1231 3548 65 241 4779 Tue ## 3 2016-04-13 117 528 1897 72 509 2425 Wed ## 4 2017-02-04 172 1357 3324 81 425 4681 Sat It’s strange: there was nothing remarkable about these days. A quick study of my Google Maps timeline shows that I didn’t go anywhere remotely interesting on any of these days, other than work. I had expected that my record usage would come with travel, but it seems that it just came with boredom. A quick look back at the photos I took on those days confirms the sheer mundanity of my record-setting Tinder marathons. I swiped 12,250 times on the days I took those photos. I will never stop laughing at this. 10.2.3 “It’s Like Batting Average, But For Tinder” Next, we will debut my favorite new statistic from this analysis: the swipe right percentage print(&#39;Swipe right percentage:&#39;) ## [1] &quot;Swipe right percentage:&quot; 100 * (sum(bentinder$likes) / (sum(bentinder$likes) + sum(bentinder$passes))) ## [1] 26.00954 I swipe right on (a.k.a. “like”) only 26% of users. At first, I thought this felt low, and that I was being too picky. Then, I wondered if this might actually be high, since I have no baseline to judge it against. So, let’s answer another simple question: what percentage of users I swiped right on (or “liked”) swiped right on me (“liked” me back)? print(&#39;Match percentage:&#39;) ## [1] &quot;Match percentage:&quot; 100 * sum(bentinder$matches) / sum(bentinder$likes) ## [1] 11.63997 This number is much lower - only 11.6%! I like 26% of users, but only 11.6% of those users like me back. However, it’s important to note that 11.6% of users like me among users that I liked. For the general population, it’s likely a higher percentage, perhaps equal to or greater than 26%. Unfortunately, Tinder does not provide data on how other users swiped you, and we cannot derive this value using probability theory without further information. Still, it’s fascinating to know that of the people I like, only about 1 in 11 like me back. Perhaps I am too picky! For good measure, let’s calculate the percentage of swipes that have yielded a match. print(&#39;Swipes per match:&#39;) ## [1] &quot;Swipes per match:&quot; 100 * sum(bentinder$matches) / sum(bentinder$swipes) ## [1] 3.027502 To be fair, this isn’t quite as bad as I thought. I can deal with swiping 33 times (which takes a minute or two) to get a match. Had this number been 100, I would have felt very differently. We will incorporate these variables into the rest of our analysis as follows. First, we add a swipe right rate, which is equal to the number of times I swipe right divided by my total number of swipes. Second, we add a match rate, a log-adjusted variable that gets higher as more users return my swipes right in kind, and lower as more of the users I liked pass on me. Additional details for math people: To be more specific, we will take the ratio of matches to swipes right, parse any zeros in the numerator or the denominator to 1 (necessary for generating real-valued logarithms), and then take the natural logarithm of this value. This statistic itself won’t be particularly interpretable, but the comparative overall trends will be. 10.2.4 Where &amp; When Did My Swiping Habits Change? We will start our graphing by examining my match rate and swipe right rate over time. bentinder = bentinder %&gt;% mutate(swipe_right_rate = (likes / (likes+passes))) %&gt;% mutate(match_rate = log( ifelse(matches==0,1,matches) / ifelse(likes==0,1,likes))) rates = bentinder %&gt;% select(date,swipe_right_rate,match_rate) match_rate_plot = ggplot(rates) + geom_point(size=0.2,alpha=0.5,aes(date,match_rate)) + geom_smooth(aes(date,match_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=-0.5,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=-0.5,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=-0.5,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(-2,-.4)) + ggtitle(&#39;Match Rate Over Time&#39;) + ylab(&#39;&#39;) swipe_rate_plot = ggplot(rates) + geom_point(aes(date,swipe_right_rate),size=0.2,alpha=0.5) + geom_smooth(aes(date,swipe_right_rate),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=.345,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=1) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=.345,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=.345,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.4) + tinder_theme() + coord_cartesian(ylim = c(.2,0.35)) + ggtitle(&#39;Swipe Right Rate Over Time&#39;) + ylab(&#39;&#39;) grid.arrange(match_rate_plot,swipe_rate_plot,nrow=2) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Match rate fluctuates very wildly over time, and there clearly isn’t any sort of annual or monthly trend. It’s cyclical, but not in any obviously traceable manner. My best guess here is that the quality of my profile photos (and perhaps general dating prowess) varied significantly over the last five years, and these peaks and valleys trace the periods when I became more or less attractive to other users. The jumps on the curve are significant, corresponding to users liking me back anywhere from about 20% to 50% of the time. Perhaps this is evidence that the perceived “hot streaks” or “cold streaks” in one’s dating life are a very real thing. Swipe right rate stays much more consistent. There are fewer peaks and valleys, and there’s less overall variation. However, there is a very noticeable dip in Philadelphia. As a native Philadelphian, the implications of this frighten me. We have routinely been derided as having some of the least attractive residents in the nation. I passionately reject that implication. I refuse to accept this as a proud native of the Delaware Valley. That being the case, I’m going to write this off as being a product of disproportionate sample sizes and leave it at that. The uptick in New York is abundantly clear across the board, though. I used Tinder very little in Summer 2019 while preparing for graduate school, which causes many of the usage rate dips we’ll see in 2019 - but there is a huge jump to all-time highs across the board when I move to New York. If you’re an LGBT millennial using Tinder, it’s difficult to beat New York. 10.2.5 A Problem With Dates If you study these tables, you’ll notice the same issue I did - missing data for messages and app opens. bentinder[1:20,-c(8,9)] ## date opens likes passes matches messages swipes ## 1 2014-11-12 0 24 40 1 0 64 ## 2 2014-11-13 0 8 23 0 0 31 ## 3 2014-11-14 0 3 18 0 0 21 ## 4 2014-11-16 0 12 50 1 0 62 ## 5 2014-11-17 0 6 28 1 0 34 ## 6 2014-11-18 0 9 38 1 0 47 ## 7 2014-11-19 0 9 21 0 0 30 ## 8 2014-11-20 0 8 13 0 0 21 ## 9 2014-12-01 0 8 34 0 0 42 ## 10 2014-12-02 0 9 41 0 0 50 ## 11 2014-12-05 0 33 64 1 0 97 ## 12 2014-12-06 0 19 26 1 0 45 ## 13 2014-12-07 0 14 31 0 0 45 ## 14 2014-12-08 0 12 22 0 0 34 ## 15 2014-12-09 0 22 40 0 0 62 ## 16 2014-12-10 0 1 6 0 0 7 ## 17 2014-12-16 0 2 2 0 0 4 ## 18 2014-12-17 0 0 0 1 0 0 ## 19 2014-12-18 0 0 0 2 0 0 ## 20 2014-12-19 0 0 0 1 0 0 print(&#39;----------skipping rows 21 to 169----------&#39;) ## [1] &quot;----------skipping rows 21 to 169----------&quot; bentinder[170:190,-c(8,9)] ## date opens likes passes matches messages swipes ## 170 2015-09-07 5 11 18 1 0 29 ## 171 2015-09-08 4 36 96 3 0 132 ## 172 2015-09-09 8 7 11 2 0 18 ## 173 2015-09-10 2 4 7 0 0 11 ## 174 2015-09-11 3 22 60 3 0 82 ## 175 2015-09-14 2 24 56 0 0 80 ## 176 2015-09-15 1 10 16 1 0 26 ## 177 2015-09-17 0 0 0 1 0 0 ## 178 2015-09-18 2 0 0 0 0 0 ## 179 2015-09-19 1 32 87 2 0 119 ## 180 2015-09-20 1 0 0 1 1 0 ## 181 2015-09-21 2 0 2 1 0 2 ## 182 2015-09-22 1 1 3 0 0 4 ## 183 2015-09-25 1 41 105 3 0 146 ## 184 2015-09-26 0 0 0 2 0 0 ## 185 2015-09-27 0 0 0 1 0 0 ## 186 2015-09-28 0 0 0 1 0 0 ## 187 2015-09-29 9 35 94 3 23 129 ## 188 2015-09-30 11 15 25 3 8 40 ## 189 2015-10-01 2 1 4 0 3 5 ## 190 2015-10-04 0 0 0 0 1 0 bentinder = bentinder %&gt;% select(-c(likes,passes,swipe_right_rate,match_rate)) bentinder = bentinder[-c(1:186),] messages = messages[-c(1:186),] We clearly cannot compile any useful averages or trends using those categories if we’re factoring in data collected before Sep 29, 2015. Therefore, we will restrict our data set to all dates since Sep 29, 2015 moving forward, and all inferences will be made using data from that date on. 10.2.6 Overall Trends Now that we’ve redefined our data set and removed our missing values, let’s examine the relationships between our remaining variables. ggduo(bentinder[2:5], types=list(continuous = wrap(&quot;smooth_loess&quot;, alpha = 0.4,size=0.2))) + tinder_theme() It’s abundantly obvious how much outliers affect this data. Nearly all the points are clustered in the lower left-hand corner of every graph. We can see general long-term trends, but it’s hard to make any sort of deeper inference. There are a lot of very extreme outlier days here, as we can see by studying the boxplots of my usage statistics. tidyben = bentinder %&gt;% gather(key = &#39;var&#39;,value = &#39;value&#39;,-date) ggplot(tidyben,aes(y=value)) + coord_flip() + geom_boxplot() + facet_wrap(~var,scales = &#39;free&#39;,nrow=5) + tinder_theme() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + ggtitle(&#39;Daily Tinder Stats&#39;) + theme(axis.text.y = element_blank(),axis.ticks.y = element_blank()) A handful of extreme high-usage dates skew our data, and will make it difficult to view trends in graphs. Thus, henceforth, we will “zoom in” on graphs, displaying a smaller range on the y-axis and hiding outliers in order to better visualize overall trends. 10.2.7 Playing Hard To Get Let’s start zeroing in on trends by “zooming in” on my message differential over time - the daily difference between the number of messages I get and the number of messages I receive. ggplot(messages) + geom_point(aes(date,message_differential),size=0.2,alpha=0.5) + geom_smooth(aes(date,message_differential),color=tinder_pink,size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=6,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=0.2) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=6,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=6,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.44) + tinder_theme() + ylab(&#39;Messages Sent/Received In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Differential Over Time&#39;) + coord_cartesian(ylim=c(-7,7)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; The left side of this graph probably doesn’t mean much, since my message differential was closer to zero when I barely used Tinder early on. What’s interesting here is I was talking more than the people I matched with in 2017, but over time that trend eroded. Either I’m talking less, people are talking to me more, or both. Let’s view messages sent and messages received separately and study the trend a little closer. tidy_messages = messages %&gt;% select(-message_differential) %&gt;% gather(key = &#39;key&#39;,value = &#39;value&#39;,-date) ggplot(tidy_messages) + geom_smooth(aes(date,value,color=key),size=2,se=FALSE) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=29,label=&#39;Pittsburgh&#39;,color=&#39;blue&#39;,hjust=.3) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=29,label=&#39;Philadelphia&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NYC&#39;,color=&#39;blue&#39;,hjust=-.2) + tinder_theme() + ylab(&#39;Msg Received &amp; Msg Sent In Day&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Message Rates Over Time&#39;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; There are a number of possible conclusions you might draw from this graph, and it’s hard to make a definitive statement about it - but my takeaway from this graph was this: I talked way too much in 2017, and over time I learned to send fewer messages and let people come to me. As I did this, the lengths of my conversations eventually reached all-time highs (after the usage dip in Phiadelphia that we’ll discuss in a second). Sure enough, as we’ll see soon, my messages peak in mid-2019 more precipitously than any other usage stat (although we will discuss other potential explanations for this). Learning to push less - colloquially known as playing “hard to get” - appeared to work much better, and now I get more messages than ever and more messages than I send. Again, this graph is open to interpretation. For instance, it’s also possible that my profile simply got better over the last couple years, and other users became more interested in me and started messaging me more. Whatever the case, clearly what I am doing now is working better for me than it was in 2017. 10.2.8 Playing The Game ggplot(tidyben,aes(x=date,y=value)) + geom_point(size=0.5,alpha=0.3) + geom_smooth(color=tinder_pink,se=FALSE) + facet_wrap(~var,scales = &#39;free&#39;) + tinder_theme() + ggtitle(&#39;Daily Tinder Stats Over Time&#39;) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; mat = ggplot(bentinder) + geom_point(aes(x=date,y=matches),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=matches),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=13,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=13,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=13,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,15)) + ylab(&#39;Matches&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Matches Over Time&#39;) mes = ggplot(bentinder) + geom_point(aes(x=date,y=messages),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=messages),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=55,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=55,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=30,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,60)) + ylab(&#39;Messages&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Messages Over Time&#39;) opns = ggplot(bentinder) + geom_point(aes(x=date,y=opens),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=opens),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=32,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=32,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=32,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,35)) + ylab(&#39;App Opens&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Tinder Opens Over Time&#39;) swps = ggplot(bentinder) + geom_point(aes(x=date,y=swipes),size=0.5,alpha=0.4) + geom_smooth(aes(x=date,y=swipes),color=tinder_pink,se=FALSE,size=2) + geom_vline(xintercept=date(&#39;2016-09-24&#39;),color=&#39;blue&#39;,size=1) + geom_vline(xintercept=date(&#39;2019-08-01&#39;),color=&#39;blue&#39;,size=1) + annotate(&#39;text&#39;,x=ymd(&#39;2016-01-01&#39;),y=380,label=&#39;PIT&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2018-02-26&#39;),y=380,label=&#39;PHL&#39;,color=&#39;blue&#39;,hjust=0.5) + annotate(&#39;text&#39;,x=ymd(&#39;2019-08-01&#39;),y=380,label=&#39;NY&#39;,color=&#39;blue&#39;,hjust=-.15) + tinder_theme() + coord_cartesian(ylim=c(0,400)) + ylab(&#39;Swipes&#39;) + xlab(&#39;Date&#39;) + ggtitle(&#39;Swipes Over Time&#39;) grid.arrange(mat,mes,opns,swps) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Even though my swipe right rate went down in Philadelphia, my usage went up (at least at first). This is probably due to Philadelphia having a much larger population than Pittsburgh, but it could also be a product of having a new dating pool after moving. That always causes a flurry of new Tinder activity. The massive dips during the second half of my time in Philadelphia undoubtedly correlates with my preparations for graduate school, which started in early 2018. Then there’s a surge upon arriving in New York and having a month off to swipe, and a significantly larger dating pool. Notice that when I move to New York, all the usage stats peak, but there is an especially precipitous rise in the length of my conversations. Sure, I had more time on my hands (which feeds growth in all these measures), but the relatively large surge in messages suggests I was making more meaningful, conversation-worthy connections than I had in the other cities. This could have something to do with New York, or maybe (as mentioned earlier) an improvement in my messaging style. 10.2.9 “Swipe Night, Part 2” Overall, there is some variation over time with my usage stats, but how much of this is cyclical? We don’t see any evidence of seasonality, but perhaps there’s variation based on the day of the week? Let’s investigate. There isn’t much to see when we compare months (cursory graphing confirmed this), but there’s a clear pattern based on the day of the week. by_day = bentinder %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(messages=mean(messages),matches=mean(matches),opens=mean(opens),swipes=mean(swipes)) colnames(by_day)[1] = &#39;day&#39; mutate(by_day,day = substr(day,1,2)) ## # A tibble: 7 x 5 ## day messages matches opens swipes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 39.7 8.43 21.8 256. ## 2 Mo 34.5 6.89 20.6 190. ## 3 Tu 30.3 5.67 17.4 183. ## 4 We 29.0 5.15 16.8 159. ## 5 Th 26.5 5.80 17.2 199. ## 6 Fr 27.7 6.22 16.8 243. ## 7 Sa 45.0 8.90 25.1 344. by_days = by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) rates_by_day = rates %&gt;% group_by(wday(date,label=TRUE)) %&gt;% summarize(swipe_right_rate=mean(swipe_right_rate,na.rm=T),match_rate=mean(match_rate,na.rm=T)) colnames(rates_by_day)[1] = &#39;day&#39; mutate(rates_by_day,day = substr(day,1,2)) ## # A tibble: 7 x 3 ## day swipe_right_rate match_rate ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Su 0.303 -1.16 ## 2 Mo 0.287 -1.12 ## 3 Tu 0.279 -1.18 ## 4 We 0.302 -1.10 ## 5 Th 0.278 -1.19 ## 6 Fr 0.276 -1.26 ## 7 Sa 0.273 -1.40 rates_by_days = rates_by_day %&gt;% gather(key=&#39;var&#39;,value=&#39;value&#39;,-day) ggplot(rates_by_days) + geom_col(aes(x=fct_relevel(day,&#39;Sat&#39;),y=value),fill=tinder_pink,color=&#39;black&#39;) + tinder_theme() + facet_wrap(~var,scales=&#39;free&#39;) + ggtitle(&#39;Tinder Stats By Day of Week&#39;) + xlab(&quot;&quot;) + ylab(&quot;&quot;) Tinder recently labeled Sunday its “Swipe Night”, but for me, that title goes to Saturday. I use the app most then, and the fruits of my labor (matches, messages, and opens that are presumably related to the messages I’m receiving) slowly cascade over the course of the week. I wouldn’t make too much of my match rate dipping on Saturdays. It can take a day or five for a user you liked to open the app, see your profile, and like you back. These graphs suggest that with my increased swiping on Saturdays, my immediate conversion rate goes down, probably for this exact reason. We’ve captured an important feature of Tinder here: it is seldom immediate. Instantaneous responses are rare on Tinder. It’s an app that involves a lot of waiting. You need to wait for a user you liked to like you back, wait for one of you to see the match and send a message, wait for that message to be returned, and so on. This can take a while. It can take days for a match to happen, and then days for a conversation to ramp up. As my Saturday numbers suggest, this often doesn’t happen the same night. So perhaps Tinder is better at looking for a date sometime this week than looking for a date later tonight. 10.2.10 For My Fellow Data Nerds, Or People Who Just Like Graphs Here’s a parallel coordinate plot that allows you to play with the outliers in my usage categories, and see how my luck varied on my high-usage days. Perhaps you’ll notice a trend that I missed. nodates = select(bentinder,-date) parcoords(nodates, rownames = F, brushMode = &quot;1D-axes&quot;, alpha = .4, reorderable = T, queue = T, color = tinder_pink) 10.3 Conclusion 10.3.1 Dubious Demographics The most profound takeaway here is that these numbers cast serious doubt upon many empirical estimates of LGBT populations. We can say with reasonable confidence that my swiping numbers are implausible if estimates of Pennsylvania’s LGBT population are to be believed. 10.3.2 Love Is Bored My Tinder usage peaked when I was doing very little, and it hits its weekly high-water mark on Saturdays. The busier I got in 2019, the more my usage plummeted. Having more time for Tinder clearly leads to more Tinder usage, and having less time for Tinder clearly leads to less Tinder usage. This seems intuitive, but it suggests that romantic obsessions may have as much to do with having nothing to do as they do with actual romance. 10.3.3 Does Location Matter? Well, Maybe. I expected larger differences between localities, but it was difficult to make precise geographic inferences. There were too many other life changes in this period to make any sweeping statements. The drop in swipe right rate upon moving to Philadelphia does stand out (much to my dismay), but certainly not as much as the peaks across the board in New York. Granted, I had a lot of time to swipe when I arrived in New York since I had a month off before school, but it seems that both the quantity and quality of my connections surged. When you’re gay, living in a big city is great for cultural reasons - but we don’t talk enough about how much overall population matters. Clearly, New York was an incredible place to swipe. Location can mean everything when you’re gay. I’ll be curious to see how these New York numbers evolve over time, and if the sheer population of the city allows me to sustain them better than I did in Philadelphia. 10.3.4 The Cinderella Effect My match rate fluctuated very wildly over time, which implies that users’ interest in me varied over time. We can think of this a couple ways - either my representation of myself changed in my profile, or I myself changed and become more or less attractive to other users. Either way, we can interpret this as a sign that we aren’t static, and a person can always get better (or worse) at dating - especially online. 10.3.5 “Playing Hard To Get” May A Be Real Thing The less I dominated my conversations, the longer they got. We can’t prove causality here, but my message differential charts make it appear that a more relaxed, succinct approach to conversation benefitted me. This brings me to my next question… 10.3.6 Can We Solve Dating Using Machine Learning? It would be fascinating to see how my success rates - namely message differential and match rate - are affected by how I use the app. Not using the app isn’t a recipe for dating success, but overdoing it and obsessively swiping and messaging isn’t a good strategy, either. So where’s the middle ground? If we build a model using this data, we may be able to answer these questions, and find an ideal way to use Tinder to maximize success. That’s beyond the scope of this study, but it’s something I’d like to explore down the line. 10.4 Final Thoughts I probably use Tinder too much. I think we’ve established that. Still though, the fruits of my time invested are abundantly evident. I’ve made 8,777 connections, 8,500 of which I probably never would have made otherwise, and some of which became very meaningful to me. We are seldom fortunate enough to have a true quantification of the number of people we’ve interacted with over the years - so as hilariously excessive as all this seems, it’s pretty cool to have such a definitive empirical trace of my 20’s. I’ve learned a lot about myself through this analysis, and I strongly encourage you to run my script and do the same for yourself with your own data. I’d love to hear what you find. Until then, swipe on, my friends. "],
["wordcloud.html", "Chapter 11 Wordcloud 11.1 1. Introduction 11.2 2. Demo of wordcloud2 Package", " Chapter 11 Wordcloud Chengyou Ju and Yujie Wang This Rmd file is created by Chengyou Ju (UNI: cj2624) and Yujie Wang (UNI: yw3442) for STAT GR5702 Community Contribution Group 15. In this file, we will provide a tutorial on how to draw Wordcloud graphs using the wordcloud2 package in R. The dataset in this project are from the demoFreq package. 11.1 1. Introduction A Wordcloud is a visual representation of text data. Wordclouds are useful for quickly perceiving the most prominent terms, which makes them widely used in media and well understood by the public. A Wordcloud is a collection of words depicted in different sizes. The bigger and bolder the word appears, the greater frequency within a given text and the more important it is. There are two packages in R that can help us draw a wordcloud. wordcloud is the basic package to build the graph, while wordcloud2 package allows more customization. In our demo, we will focus on the wordcloud2 package, which is more widely used. 11.2 2. Demo of wordcloud2 Package For our demo, we will use a built-in dataset demoFreq, which has 1011 observations of 2 variables, words and frequancy. library(devtools) ## Loading required package: usethis devtools::install_github(&quot;lchiffon/wordcloud2&quot;) ## Downloading GitHub repo lchiffon/wordcloud2@master ## ## checking for file ‘/tmp/Rtmpu9OuqD/remotes4d741b9c2d13/Lchiffon-wordcloud2-8a12a3b/DESCRIPTION’ ... ✔ checking for file ‘/tmp/Rtmpu9OuqD/remotes4d741b9c2d13/Lchiffon-wordcloud2-8a12a3b/DESCRIPTION’ ## ─ preparing ‘wordcloud2’: ## checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ## ─ checking for LF line-endings in source and make files and shell scripts ## ─ checking for empty or unneeded directories ## Removed empty directory ‘wordcloud2/examples/img’ ## Removed empty directory ‘wordcloud2/examples’ ## ─ looking to see if a ‘data/datalist’ file should be added ## ─ building ‘wordcloud2_0.2.2.tar.gz’ ## ## ## Installing package into &#39;/home/travis/R/Library&#39; ## (as &#39;lib&#39; is unspecified) library(wordcloud2) head(demoFreq) ## word freq ## oil oil 85 ## said said 73 ## prices prices 48 ## opec opec 42 ## mln mln 31 ## the the 26 Parameters for wordcloud2 from Rdocumentation data - data frame with word and freqency of the word size - Font size, default is 1. The larger size means the bigger word fontFamily - font used in the word cloud fontWeight - Font weight to use, e.g. normal, bold or 600 color - color of the text, keyword ’random-dark’ and ’random-light’ can be used. backgroundColor - Color of the background minRotation - If the word should rotate, the minimum rotation (in rad) the text should rotate. maxRotation - If the word should rotate, the maximum rotation (in rad) the text should rotate. shuffle - Shuffle the points to draw so the result will be different each time for the same list and settings. rotateRatio - Probability for the word to rotate. Set the number to 1 to always rotate. shape - The shape of the “cloud” to draw. Can be a keyword present. widgetsize - size of the widgets figPath - The path to a figure used as a mask. hoverFunction - Callback to call when the cursor enters or leaves a region occupied by a word. 11.2.1 2.0 Basic Wordcloud Graph Building a wordcloud graph is simple. We can use the wordcloud2 package directly after successfully installing it. wordcloud2(data = demoFreq) As we can see, the word cloud is easy to build and to read. Words with large frequency like ‘said’ and ‘oil’ are displayed in big font size. It is actually an interactive plot. If we hover on a certain word, it will pop up the word with its frequency. 11.2.2 2.1 Font Size We can also modify the font size of the graph. wordcloud2(data = demoFreq, size = 0.5) wordcloud2(data = demoFreq, size = 1.5) 11.2.3 2.2 Color and Background Color The word color can be changed using the “color” argument, while the background color can be changed with “backgroundColor”. wordcloud2(demoFreq, color = &#39;random-dark&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;) wordcloud2(demoFreq, color = &#39;random-light&#39;, backgroundColor = &quot;black&quot;) wordcloud2(demoFreq, color = rep_len(c(&quot;skyblue&quot;, &quot;blue&quot;), nrow(demoFreq))) 11.2.4 2.3 Shape We can also customize the shape of a wordcloud using the “shape” argument. Here are some examples. wordcloud2(demoFreq, size = 0.5, shape = &#39;star&#39;) wordcloud2(demoFreq, size = 0.5, shape = &#39;pentagon&#39;) 11.2.5 2.4 Rotation We can also do rotation on the wordcloud graph. wordcloud2(demoFreq, minRotation = -pi/6, maxRotation = -pi/6, rotateRatio = 1) 11.2.6 2.5 Language We can draw a wordcloud graph of words in Chinese. wordcloud2(demoFreqC, fontFamily = &quot;????????????&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) 11.2.7 2.6 Customized shape We can build wordcloud with the shape of a word using function letterCloud. letterCloud(demoFreq, word = &quot;R&quot;, color = &quot;random-light&quot;, backgroundColor = &quot;black&quot;) Also, we can create user-defined shape for the wordcloud by simply adding the image we choose to figPath. wordcloud2(demoFreq, figPath =&quot;~/Desktop/batman.png&quot;, size = 1, color = &quot;random-light&quot;,backgroundColor = &quot;black&quot;) batman "],
["latex-visualization.html", "Chapter 12 Latex Visualization", " Chapter 12 Latex Visualization Yuki Nishimura and Jay Zern Ng 12.0.1 Summary LaTeX Visualization is essential for writers that are hoping to publish academic papers with visualizations of concepts and findings. For first time academic writers, it may be difficult to figure out the types of visualizations that could be done using LaTeX. Therefore, we have written a tutorial for LaTeX Visualization to accommodate the needs of inexperienced academic writers in the following link: Click Here Hope you enjoy the tutorial :D "],
["ggmosaic.html", "Chapter 13 ggmosaic 13.1 Overview 13.2 Introduction 13.3 Order of splits 13.4 Splitting on One Variable(binned data) 13.5 Splitting on One Variable(unbinned data) 13.6 Splitting on Two Variables 13.7 Splitting on Three Variables 13.8 Adjusting the Direction of Splits 13.9 Alternative approach: Conditional 13.10 Alternative approach: Facetting 13.11 Comparison with vcd::mosaic 13.12 Chinese Translation: ‘ggmosaic’（马赛克图） 13.13 引言 13.14 简介 13.15 分割的顺序 13.16 根据一个变量分割（分箱数据）： 13.17 根据一个变量分割(非分箱数据): 13.18 根据两个变量分割 13.19 根据三个变量分割 13.20 调整切割的方向 13.21 另外一种方法:条件变量(Conditional) 13.22 另外一种方法:块化(Facet) 13.23 ‘ggmosaic’ vs vcd::‘mosaic’", " Chapter 13 ggmosaic Qiang Zhao Mike Yao-Yi Wang library(rmarkdown) library(rticles) 13.1 Overview This cheat sheet is inspired by the Chapter 15 Chart: Mosaic of the edav.info. Instead of using the mosaic function from the package vcd to plot the mosaic plot, this cheat sheet shows how to achieve the same output through using ggmosaic. 13.2 Introduction Mosaic plot is only for categorical data Variables to put in geom_mosaic: weight: Count/Freq column x: product(Y, X2, X1) fill: dependent variable Y conds: conditional variable 13.3 Order of splits The mosaic plot follows the hierarchical structure, thus the order of adding variables is very important. Below we will show a step by step splitting by adding one variable at the time. Before going through the example, one must install and call the package ggplot2 and ggmosaic. library(ggplot2) library(ggmosaic) df_bin=data.frame(Age=c(&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;old&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;,&#39;young&#39;), Favorite=c(rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2),rep(&#39;bubble gum&#39;,2),rep(&#39;coffee&#39;,2)), Music=c(rep(c(&#39;classical&#39;,&#39;rock&#39;),4)), Freq=c(1,1,3,1,2,5,1,0)) df_unbin = data.frame(Age =c(rep(&quot;old&quot;,6), rep(&quot;young&quot;, 8)), Favorite = c(rep(&quot;bubble gum&quot;, 2),rep(&quot;coffee&quot;, 4), rep(&quot;bubble gum&quot;, 7), &quot;coffee&quot;), Music = c(&quot;classical&quot;, &quot;rock&quot;, rep(&quot;classical&quot;, 3), &quot;rock&quot;, rep(&quot;classical&quot;, 2), rep(&quot;rock&quot;, 5), &quot;classical&quot;)) 13.4 Splitting on One Variable(binned data) df_bin ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 ## 7 young coffee classical 1 ## 8 young coffee rock 0 First, we will show the ggmosaic only split on Age: Important: The ggmosaic can take binned data by assigning the weight = Freq column of the dataset at its aesthetics, it is not like vcd::mosaic(), which can only take binned data with count column name as Freq. ggplot(data = df_bin)+ geom_mosaic(aes(x = product(Age), fill = Age, weight = Freq))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(binned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.5 Splitting on One Variable(unbinned data) However, for unbinned data, we could just ignore the weight and let it set to default. The unbinned data: df_unbin ## Age Favorite Music ## 1 old bubble gum classical ## 2 old bubble gum rock ## 3 old coffee classical ## 4 old coffee classical ## 5 old coffee classical ## 6 old coffee rock ## 7 young bubble gum classical ## 8 young bubble gum classical ## 9 young bubble gum rock ## 10 young bubble gum rock ## 11 young bubble gum rock ## 12 young bubble gum rock ## 13 young bubble gum rock ## 14 young coffee classical ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age), fill = Age))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(unbinned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note: We will use unbinned data for the rest of example 13.6 Splitting on Two Variables Split on Age, then Music: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Music, Age), fill = Music))+ labs(x = &quot;Age&quot;, y = &quot;Music&quot;, title = &quot;Spliting on Age, then Music&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Split on Music, then Age: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age, Music), fill = Age))+ labs(x= &quot;Music&quot;, y = &quot;Age&quot;, title = &quot;Spliting on Music, then Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) For plotting mosaic plot on Y ~ X, we want to set x = product(Y, X) in aes as we always want to split the dependent variable last. We also need to set fill = Y as we want to color base on dependent variable. 13.7 Splitting on Three Variables Split on Age, then Music, then Favorite: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite))+ labs(x = &quot;Favorite:Age&quot;, y = &quot;Music&quot;, title = &quot;Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note that in the above example, by default the order of split and their directions as follows: Age – vertical split Music – horizontal split Favorite – vertical split 13.8 Adjusting the Direction of Splits The directions can be adjusted as we want. For example, we want to create a doubledecker plot for the above example following below criteria: Splitting order: Age – vertical split (“hspine”) Music – vertical split (“hspine”) Favorite (dependent variable)– horizontal split (“vspine”) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) Note that the divider vector is in order of which the variables appear in the product(Favorite, Music, Age), however the order of splits is Age, Music, then Favorite. Also note that in the divider vector, “vspine” = horizontal split and “hspine” = vertical split. 13.9 Alternative approach: Conditional We can also use conditional property to achieve the same result as the above. In this case, geom_mosaic(aes(x = product(last_split), fill = last_split, conds = product(second_split, first_split)). ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite), fill = Favorite, conds = product(Music, Age)), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - (Favorite | Music, Age)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.10 Alternative approach: Facetting We can also achieve similar result through facetting. ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music), fill = Favorite))+ facet_grid(. ~Age)+ labs(x=&quot;Music&quot;, y = &quot;favorite&quot;, title = &quot;Favorite ~ Music and facet on Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.11 Comparison with vcd::mosaic There are often confusions between ggmosaic:geom_mosaic and vcd:mosaic as the syntax for splitting order and splitting direction are quite different for the two. The vcd:mosaic follows the order of mosaic(last_split ~ first_split + second_split) and the direction vector in the order of splits is (first_split, second_split, third_split) with “v” being the vertical split and “h” being the horizontal split. However, ggmosaic:geom_mosaic follow the different pattern, the order of split is product(last_split, second_split, first_split) and the divider (similar to direction in vcd:mosaic) in the order of split is divider = c(last_split, second_split, first_split) with “vspine” being the horizontal split and “hspine” being the vertical split. 13.12 Chinese Translation: ‘ggmosaic’（马赛克图） 13.13 引言 这个文档是参考edav.info中第十五章节：马赛克图（Mosaic），引用其中的数据和例子。相较于edav.info中vcd包里的mosiac函数，我们准备使用ggmosiac来画马赛克图。 13.14 简介 马赛克图针对的是分类变量 geom_maisc中的变量 weight : 数据中的计数栏 x : product（因变量，自变量2，自变量1） fill : 数据中的自变量栏 conds : 条件变量 13.15 分割的顺序 马赛克图遵循等级分层结构，因此往product里面加变量的顺序极其重要。下面，我们会一步一步的展示如何正确的加入变量。我们首先要安装并且引用ggplot2和ggmosaic包。 library(ggplot2) library(ggmosaic) 13.16 根据一个变量分割（分箱数据）： df_bin ## Age Favorite Music Freq ## 1 old bubble gum classical 1 ## 2 old bubble gum rock 1 ## 3 old coffee classical 3 ## 4 old coffee rock 1 ## 5 young bubble gum classical 2 ## 6 young bubble gum rock 5 ## 7 young coffee classical 1 ## 8 young coffee rock 0 首先，我们根据年龄（Age）分割： 注意：ggmosaic可以通过weight来处理分箱数据，我们令weight等于数据中的计数栏（Freq）即可。vcd::mosaic也可以做到同样效果，但是计数栏的名称一定要为Freq。 ggplot(data = df_bin)+ geom_mosaic(aes(x = product(Age), fill = Age, weight = Freq))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(binned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.17 根据一个变量分割(非分箱数据): 对于非分箱数据，我们应该忽略weight。下面是非分箱数据的读数: df_unbin ## Age Favorite Music ## 1 old bubble gum classical ## 2 old bubble gum rock ## 3 old coffee classical ## 4 old coffee classical ## 5 old coffee classical ## 6 old coffee rock ## 7 young bubble gum classical ## 8 young bubble gum classical ## 9 young bubble gum rock ## 10 young bubble gum rock ## 11 young bubble gum rock ## 12 young bubble gum rock ## 13 young bubble gum rock ## 14 young coffee classical ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age), fill = Age))+ labs(x= &quot;Age&quot;, title = &quot;Spliting on Age(unbinned data)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：我们接下来的例子都是使用非分箱的数据 13.18 根据两个变量分割 我们首先根据年龄(Age)分割，然后再根据音乐种类(Music)分割: ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Music, Age), fill = Music))+ labs(x = &quot;Age&quot;, y = &quot;Music&quot;, title = &quot;Spliting on Age, then Music&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 下面这个例子是先分割音乐种类(Music)，再分割年龄(Age): ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Age, Music), fill = Age))+ labs(x= &quot;Music&quot;, y = &quot;Age&quot;, title = &quot;Spliting on Music, then Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 如果我们想画因变量Y关于自变量X的马赛克图，我们应该设aes中的x=prod(Y,X)。我们要保证因变量是最后一个被划分的。而且, 我们也要使得fill=Y，因为我们注重的是因变量Y的分布。 13.19 根据三个变量分割 我们首先划分年龄(Age),然后划分音乐种类(Music)，最后划分喜好(Favorite)。 ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite))+ labs(x = &quot;Favorite:Age&quot;, y = &quot;Music&quot;, title = &quot;Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：在上面的例子当中，系统默认的切割方向以及顺序如下: 年龄(Age): 垂直切割 音乐种类(Music): 横向切割 喜好(Favorite): 垂直切割 13.20 调整切割的方向 我们可以随意改变任意变量的切割方向。比如，我们打算用上面的例子绘画一个双层结构图(DoubleDecker Plot)。 切割顺序为： 年龄(Age): 垂直切割 (‘hspine’) 音乐种类(Music): 垂直切割 (‘hespine’) 喜好(Favorite): 横向切割 (‘vspine’) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music, Age), fill = Favorite), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - Split on Age, then Music, then Favorite&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 注意：divider中的切割方向分别对应product(Favorite,Music,Age)。但是实际上的切割顺序还是年龄(Age)，音乐种类(Music)，喜好(Favorite)。divider中的’vspine’表示横向切割；’hspine’表示垂直切割。 13.21 另外一种方法:条件变量(Conditional) 我们可以使用条件(conds)的属性来达到与上面例子相同的效果。 geom_mosaic(aes(x = product(最后一次切割), fill = 最后一次切割, conds = product(第二次切割, 第一次切割)). ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite), fill = Favorite, conds = product(Music, Age)), divider = c(&quot;vspine&quot;, &quot;hspine&quot;, &quot;hspine&quot;))+ labs(x = &quot;Music:Age&quot;, y = &quot;Favorite&quot;, title = &quot;Doubledecker Plot - (Favorite | Music, Age)&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.22 另外一种方法:块化(Facet) ggplot(data = df_unbin)+ geom_mosaic(aes(x = product(Favorite, Music), fill = Favorite))+ facet_grid(. ~Age)+ labs(x=&quot;Music&quot;, y = &quot;favorite&quot;, title = &quot;Favorite ~ Music and facet on Age&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 13.23 ‘ggmosaic’ vs vcd::‘mosaic’ 我们很容易搞混’ggmosaic’和‘mosaic’的切割顺序和切割方向。 对于‘mosaic’来说，切割顺序服从公式’mosaic(最后一个切割~第一个切割+第二个切割)’，而且切割方向的变量也是对应着direction=(第一个切割的变量，第二个切割的变量，最后一个切割的变量)。其中’v’表示的是垂直切割,’h’表示的是横向切割。 但是这和‘ggmosaic’有着很大的不同。在‘ggmosaic’中，切割应服从’product(最后一个切割，第二个切割，第一个切割)’的一个倒叙顺序。而且切割方向的变量也对应着divider=(最后一个切割的变量，第二个切割的变量，第一个切割的变量)。其中’vspine’表示横向切割，‘hspine’表示垂直切割。 "],
["web-scraping-using-rvest.html", "Chapter 14 Web scraping using rvest 14.1 1 Overview 14.2 2 An Easy Example 14.3 3 HTML Basics 14.4 4 Rvest 14.5 5 More Examples 14.6 6 External Resources", " Chapter 14 Web scraping using rvest Huiyu Song and Xiao Ji 14.1 1 Overview This section covers how to conduct web scraping using “rvest” package 14.2 2 An Easy Example I want an example now! Here is an example of scraping the price and percentage change of trending stocks from Yahoo Finance: https://finance.yahoo.com/trending-tickers. The first thing we need to do is to check if scraping is permitted on this page using paths_allowed( ) function. library(robotstxt) paths_allowed(paths=&quot;https://finance.yahoo.com/trending-tickers&quot;) ## finance.yahoo.com ## [1] TRUE The output is TRUE meaning that bots are allowed to access this path. Now we can scrape the data: library(rvest) TrendTicker &lt;- read_html(&quot;https://finance.yahoo.com/trending-tickers&quot;) #read the path #We need Name, Last Price, % Change Name &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col1&quot;)%&gt;%html_text() Price &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col2&quot;)%&gt;%html_text() Change &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col5&quot;)%&gt;%html_text() dt&lt;-tibble(Name,Price,Change) #combine the scrapped columns into a tibble head(dt,5) ## # A tibble: 5 x 3 ## Name Price Change ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Berkshire Hathaway Inc. 215.83 +1.53% ## 2 Bitcoin USD 9,313.80 +0.52% ## 3 Berkshire Hathaway Inc. 323,400.00 +1.40% ## 4 Saratoga Investment Corp. 25.27 +1.08% ## 5 Costco Wholesale Corporation 296.09 -0.34% Path %&gt;% html_nodes( ) %&gt;% html_text( ) is a common syntax to scrape html text and more details will be discussed in section 4. Before that, we need some basic knowledge of HTML structures. 14.3 3 HTML Basics 14.3.1 3.1 Access the source code Move your cursor to the element whose source code you want to check and right click. Select “Inspect” The source code will be displayed on the top right corner of the screen. 14.3.2 3.2 HTML structures HTML is a markup language and it describes the structure of a Web page. A simple element in HTML looks like this: div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} &lt;p&gt;This is a paragraph.&lt;/p&gt; An HTML element usually consistes of a start tag, a end tag and the content in between. Here &lt;p&gt; is the start tag, &lt;/p&gt; is the end tag (the slash indicates that it is a closing tag), “This is a paragraph” is the content. The charater “p” represents it is a paragraph element, other kinds of elements include: div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;} &lt;html&gt;: the root element of an HTML page &lt;head&gt;: an element contains meta information about the document &lt;title&gt;: an element specifies a title for the document &lt;body&gt;: an element contains the visible page content &lt;h1&gt;: an element defines a large heading &lt;p&gt;: an element defines a paragraph The basic structure of a webpage looks like this: More details can be refered to https://www.w3schools.com/html/html_intro.asp 14.4 4 Rvest When we want to scrape certain information from a website, we need to concentrate on the part that we are interested in instead of the whole page. That is why we need html_node or html_nodes to locate the interested part. 14.4.1 4.1 html_nodes and html_node Usage html_nodes(x,css,xpath) html_node(x,css,xpath) Arguments x: a node set or a single node css, xpath: Node to select css:CSS selector; xpath:XPath 1.0 selector html_node VS html_nodes Html_nodes always return a nodeset of the same length, which contains information of a set of nodes. While html_node return exactly one html_node. Here is an example: paths_allowed(&quot;https://www.eyeconic.com/&quot;) ## www.eyeconic.com ## [1] TRUE page=read_html(&quot;https://www.eyeconic.com/contact-lenses?cid=ps:google:Eyeconic+-+US+-+SKWS+-+Contacts+-+General+-+Exact+-+Geo:NB+-+Contacts+-+Onlineutm_campaign=skws&amp;ds_rl=1239071&amp;gclid=EAIaIQobChMImpP2gqW95QIVipOzCh1XfwKbEAAYAiAAEgLWrfD_BwE&amp;gclsrc=aw.ds&quot;) page2=read_html(&quot;https://www.eyeconic.com/contact-lenses/aot/AOT.html&quot;) node&lt;- page%&gt;%html_node(xpath=&#39;//*[@id=&quot;search-result-items&quot;]/li[1]&#39;) nodes&lt;-page%&gt;%html_nodes(xpath=&#39;//*[@id=&quot;search-result-items&quot;]/li[1]&#39;) node ## {html_node} ## &lt;li class=&quot;grid-tile col-md-6 col-xl-4 pb-3 px-1 px-md-2&quot; data-colors-to-show=&quot;&quot;&gt; ## [1] &lt;script type=&quot;text/javascript&quot;&gt;//&lt;!--\\n/* &lt;![CDATA[ */\\n(function(){ ... ## [2] &lt;script type=&quot;text/javascript&quot;&gt;//&lt;!--\\n/* &lt;![CDATA[ (viewProduct-act ... ## [3] &lt;div class=&quot;product-tile w-100 m-auto text-center pt-5 bg-white posi ... nodes ## {xml_nodeset (1)} ## [1] &lt;li class=&quot;grid-tile col-md-6 col-xl-4 pb-3 px-1 px-md-2&quot; data-color ... 14.4.2 4.2 css and xpath Although the usage of html_nodes and html_node seems easy and convinient, for those who cannot extract right css or xpath, the function will not work. Here is a summary of how to write css or xpath, and some examples are shown. css CSS Selector are how you pick which element to apply styles to. Selector Syntax Pattern Meaning p Select all &lt;p\\&gt; elements p m Select all &lt;m\\&gt; inside of &lt;p&gt; p &gt; m Select an direct child &lt;m&gt; of &lt;p&gt; p + m Select an &lt;m&gt; that directly follows &lt;p&gt; p ~ m Select &lt;m&gt; that preceds by &lt;p&gt; p#id_name Select all &lt;p&gt; which id=“id_name” p.class_name Select all &lt;p&gt; which class=“class_name” p[attr] Select &lt;p&gt; that has “attr” attribute p[attr=“tp”] Select &lt;p&gt; that attribute attr=“tp” p[attr~=“tp”] Select &lt;p&gt; that attribute “attr” is a list of whitespace-seperated values, and one of which is “tp” p[attr^=“tp”] Select p whose sttribute “attr” begins exactly with string “tp” p[attr*=“tp”] Select p whose sttribute “attr” contains string “tp” p[attr$=“tp”] Select p whose sttribute “attr” ends exactly with string “tp” p:root Select root of &lt;p&gt; p:nth-child(n) Select nth child of p p:nth-last-child(n) Select nth child from the bottom of p p:first-child Select first child of p p:last-child Select last child of p p:nth-of-type Select nth &lt;p&gt; in any element p:nth-last-type Select nth &lt;p&gt; from the bottom in any element p:first-of-type Select first &lt;p&gt; in any element p:last-of-type Select first &lt;p&gt; from the bottom in any element p:empty Select &lt;p&gt; that has no children p:link Select p which has not yet been visited p:visited Select p already been visited Examples p#id_character item Select any item inside p which has id=“id_character” Select name of all products. info&lt;-page%&gt;% html_nodes(&#39;ul#search-result-items li span[itemprop=&quot;name&quot;]&#39;)%&gt;% html_text() info[1:6] ## [1] &quot;ACUVUE&quot; &quot;ACUVUE&quot; &quot;AIR OPTIX&quot; &quot;ACUVUE&quot; &quot;BIOFINITY&quot; &quot;ACUVUE&quot; p.class_name Select &lt;p&gt; element which has class=“class_character”. Except id, we can also use class to concentrate on certain information. Select image path of all products. acuvue&lt;-page%&gt;% html_nodes(&#39;li.grid-tile.col-md-6.col-xl-4.pb-3.px-1.px-md-2 img[itemprop=&quot;image&quot;]&#39;) acuvue[1:2] ## {xml_nodeset (2)} ## [1] &lt;img class=&quot;w-100&quot; itemprop=&quot;image&quot; src=&quot;https://www.eyeconic.com/dw ... ## [2] &lt;img class=&quot;w-100&quot; itemprop=&quot;image&quot; src=&quot;https://www.eyeconic.com/dw ... Tips: when class name is long and has some white-spaces inside, such as “class=”product-tile w-100 m-auto text-center pt-5 bg-white position-relative“, in html class types, string between white spaces is one class, and if a class name has many whitespaces means it has many classes. Therefore to scrape those data, we need to add”.&quot; to substitute those whitespaces. A,B,C Select all &lt;A&gt;, &lt;B&gt;, &lt;C&gt; elements. Example: Scrape all product names and detail names in the page. name&lt;-page%&gt;% html_nodes(&#39;ul#search-result-items li span[itemprop=&quot;name&quot;], ul#search-result-items li div[itemprop=&quot;name&quot;]&#39;)%&gt;% html_text() name[1:6] ## [1] &quot;ACUVUE&quot; &quot;ACUVUE Oasys For Astigmatism 6pk&quot; ## [3] &quot;ACUVUE&quot; &quot;ACUVUE 1-Day Moist 90pk&quot; ## [5] &quot;AIR OPTIX&quot; &quot;Air Optix Aqua 6pk&quot; p * Select all elements in p. Select all nodes for price. img&lt;-page2%&gt;% html_nodes(&quot;div.price-info *&quot;) img ## {xml_nodeset (4)} ## [1] &lt;div class=&quot;price-standard w-auto d-inline-block&quot; data-price=&quot;54.99&quot; ... ## [2] &lt;div class=&quot;perbox-info d-inline-block text-uppercase&quot;&gt;\\n\\t\\t\\t\\t\\t&lt; ... ## [3] &lt;span class=&quot;d-block&quot;&gt;per&lt;/span&gt; ## [4] &lt;span class=&quot;d-block&quot;&gt;box of 6&lt;/span&gt; p:nth-child(n) Select nth child of &lt;p&gt; air&lt;-page%&gt;% html_nodes(&quot;ul#search-result-items:nth-child(1)&quot;) air ## {xml_nodeset (1)} ## [1] &lt;ul id=&quot;search-result-items&quot; class=&quot;search-result-items tiles-contai ... Xpath XPath (XML Path Language) uses path expressions to select nodes or node-sets in an XML document. These path expressions look very much like the expressions you see when you work with a traditional computer file system. Xpath Syntax In XPath, there are seven kinds of nodes: element, attribute, text, namespace, processing-instruction, comment, and document nodes. For example: &lt;bookstore&gt; (root element node) &lt;author&gt;J K. Rowling&lt;/author&gt; (element node) lang=“en” (attribute node) Pattern Meaning nodename Select all node with the name “nodename” A/B Select B from root node A//B Select B in the document from the current node that match the selection no matter where they are .A Select the current node A ..A Select the root of current node A @ Select attributes * Matches any element node @* Matches any attribute node node() Matches any node of any kind ancestor Select all ancestors(parent, grandparent, stc.) of the current node ancestor-of-self Select all ancestors(parent, grandparent, stc.) of the current node and current node itself attribute Select all attributes of the current node child Select all children of the current node descendant Select all descendant(children, grandchildren, etc.) of the current node following Select everything in the document after the closing tag of the current node namespace Select all namespace nodes of the current node | Select two nodes A Simple Way the get XPath right click–&gt;Copy–&gt;Copy XPath Examples Extract all product details in the contact links. library(stringr) data&lt;-data.frame() info&lt;-page%&gt;% html_nodes(&#39;ul#search-result-items li div span[itemprop=&quot;url&quot;]&#39;)%&gt;% html_text() info[1:6] ## [1] &quot;https://www.eyeconic.com/contact-lenses/aot/AOT.html&quot; ## [2] &quot;https://www.eyeconic.com/contact-lenses/a1m9/A1M9.html&quot; ## [3] &quot;https://www.eyeconic.com/contact-lenses/aos/AOS.html&quot; ## [4] &quot;https://www.eyeconic.com/contact-lenses/acuvue/OTR.html&quot; ## [5] &quot;https://www.eyeconic.com/contact-lenses/bf/BF.html&quot; ## [6] &quot;https://www.eyeconic.com/contact-lenses/ao12/AO12.html&quot; info2=as.data.frame(info) info2&lt;-mutate(info2,name=&quot;&quot;,detail=&quot;&quot;) i=1 for (link in info) { page_tp=read_html(link) details&lt;-page_tp%&gt;%html_nodes(xpath=&#39;//*[@id=&quot;contactLensPDP&quot;]/div/div[1]/div/div[2]/div[1]/p/span&#39;)%&gt;%html_text() name&lt;-page_tp%&gt;%html_nodes(xpath=&#39;//*[@id=&quot;contactLensPDP&quot;]/div/div[2]/div/div[2]/div/h1&#39;)%&gt;%html_text() info2[i,2]=name info2[i,3]=details i=i+1 } info2=info2[,-1] head(info) ## [1] &quot;https://www.eyeconic.com/contact-lenses/aot/AOT.html&quot; ## [2] &quot;https://www.eyeconic.com/contact-lenses/a1m9/A1M9.html&quot; ## [3] &quot;https://www.eyeconic.com/contact-lenses/aos/AOS.html&quot; ## [4] &quot;https://www.eyeconic.com/contact-lenses/acuvue/OTR.html&quot; ## [5] &quot;https://www.eyeconic.com/contact-lenses/bf/BF.html&quot; ## [6] &quot;https://www.eyeconic.com/contact-lenses/ao12/AO12.html&quot; 14.5 5 More Examples 14.5.1 5.1 Scrape links using attributes HTML links are defined with the tag &lt;a&gt;. The link address is specified in the “href” attribute. Suppose we want to get the link of each trend ticker, we can right click the stock symbol and check the source code: So we use “.data-col0 a”&quot; as the node and “href” as the attribute: local_links &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col0 a&quot;)%&gt;%html_attr(&quot;href&quot;) link_names &lt;- TrendTicker%&gt;% html_nodes(&quot;.data-col0 a&quot;)%&gt;%html_text(&quot;href&quot;) #complete the full link full_links=NULL for (i in 1 : length(local_links)){ full_links[i]=paste0(&quot;https://finance.yahoo.com&quot;,local_links[i]) } dt=tibble(link_names,full_links) head(dt,5) ## # A tibble: 5 x 2 ## link_names full_links ## &lt;chr&gt; &lt;chr&gt; ## 1 BRK-B https://finance.yahoo.com/quote/BRK-B?p=BRK-B ## 2 BTC-USD https://finance.yahoo.com/quote/BTC-USD?p=BTC-USD ## 3 BRK-A https://finance.yahoo.com/quote/BRK-A?p=BRK-A ## 4 SAR https://finance.yahoo.com/quote/SAR?p=SAR ## 5 COST https://finance.yahoo.com/quote/COST?p=COST 14.5.2 5.2 Scrape Table The first step is to locate the table. Then copy the Xpath. When we paste the path, it should be like: //*[@id=&quot;quote-summary&quot;]/div[1]/table Also, we need the html_table( ) function to convert the html table into a data frame: testlink=read_html(&quot;https://finance.yahoo.com/quote/TIF?p=TIF&quot;) table&lt;-testlink%&gt;% html_nodes(xpath=&#39;//*[@id=&quot;quote-summary&quot;]/div[1]/table&#39;)%&gt;% html_table() table ## [[1]] ## X1 X2 ## 1 Previous Close 124.51 ## 2 Open 124.66 ## 3 Bid 126.14 x 1100 ## 4 Ask 127.00 x 1200 ## 5 Day&#39;s Range 124.10 - 127.52 ## 6 52 Week Range 73.04 - 130.40 ## 7 Volume 3,753,122 ## 8 Avg. Volume 7,981,633 14.6 6 External Resources HTML Structure References https://www.w3schools.com/html/html_intro.asp XPath References https://en.wikipedia.org/wiki/XPath https://www.w3schools.com/xml/xml_xpath.asp CSS Selector References https://www.rdocumentation.org/packages/rvest/versions/0.3.4/topics/html_nodes http://flukeout.github.io/ https://www.w3schools.com/cssref/sel_firstchild.asp "],
["rstudio-vs-jupyterlab-for-data-science.html", "Chapter 15 RStudio vs JupyterLab for Data Science", " Chapter 15 RStudio vs JupyterLab for Data Science Dhananjay Deshpande This presentation introduces RStudio and JupyterLab as Data Science tools to the audience and compares the salient features of each tool. It later summarizes the findings and makes recommendations for Data Scientists based on current features available. RStudio vs JupyterLab for Data Science "],
["dplyr-relational-databases.html", "Chapter 16 Dplyr Relational Databases 16.1 1.Overview 16.2 2.Definition of Relational Databases 16.3 3. R Packages 16.4 4. Data description for example 16.5 5. Types of joins", " Chapter 16 Dplyr Relational Databases Alberto Munguia and Chengyi Chen 16.1 1.Overview This project intends to illustrate the use of some libarries that are relevant to manipulate and transform relational databases. 16.2 2.Definition of Relational Databases A relational database is a type of database that stores and provides access to data points that are related to one another. Relational databases are based on the relational model, an intuitive, straightforward way of representing data in tables. The relational model organizes data into one or more tables (or “relations”) of columns and rows, with a unique key identifying each row. Key plays an important role in relational database. Primary Key a.. A primary is a column or set of columns in a table that uniquely identifies tuples (rows) in that table. Super Key a.. A super key is a set of one or more columns (attributes) to uniquely identify rows in a table. Candidate Key a.. A super key with no redundant attribute is known as a candidate key. Alternate Key a.. Out of all candidate keys, only one gets selected as primary key, remaining keys are known as alternate or secondary keys. Composite Key a.. A key that consists of more than one attribute to uniquely identify rows (also known as records &amp; tuples) in a table is called composite key. Foreign Key a.. Foreign keys are the columns of a table that points to the primary key of another table. They act as a cross-reference between tables. Standard relational databases enable users to manage predefined data relationships across multiple databases. Popular examples of relational databases include Microsoft SQL Server, Oracle Database, MySQL and IBM DB2. 16.3 3. R Packages R offers flexibility in the manipulation of relational of databases through some specific functions embedded in the packages like: dplyr base sqldf Nevertheless, the data manipulation in R is easier with dplyr because the package is oriented towards the data analysis. Furthemore, dplyr offers some advantages in the join functions in comaprison with base and sqldf: For large amounts of data joining tables is faster. Rows are kept in existing order. Tells users what keys you are merging by. Work with database tables. For our example you need to install the next packages: For the dplyr manipulation. install.packages('dplyr') For the SQL manipulation. install.packages('sqldf') install.packages('gsubfn') install.packages('proto') install.packages('RSQLite') For getting the data for the example install.packages('BIS') Load the libraries library(dplyr) library(sqldf) library(gsubfn) library(proto) library(RSQLite) library(BIS) 16.4 4. Data description for example 16.4.1 4.1 BIS Library We will explore relational data from the recent data package BIS that provides an interface to data provided by the Bank for International Settlements https://www.bis.org, allowing for programmatic retrieval of a large quantity of (central) banking data. 16.4.2 4.2 Selected data sets First, we are going to upload the tables that are available in the BIS package. datasets=get_datasets() datasets ## # A tibble: 22 x 2 ## name url ## &lt;chr&gt; &lt;chr&gt; ## 1 Locational banking statistics https://www.bis.org/statistics/full_bi… ## 2 Consolidated banking statistics https://www.bis.org/statistics/full_bi… ## 3 Debt securities statistics https://www.bis.org/statistics/full_bi… ## 4 Credit to the non-financial sec… https://www.bis.org/statistics/full_bi… ## 5 Credit-to-GDP gaps https://www.bis.org/statistics/full_we… ## 6 Debt service ratios for the pri… https://www.bis.org/statistics/full_bi… ## 7 Global liquidity indicators https://www.bis.org/statistics/full_bi… ## 8 Exchange-traded derivatives sta… https://www.bis.org/statistics/full_bi… ## 9 OTC derivatives outstanding https://www.bis.org/statistics/full_bi… ## 10 US dollar exchange rates (month… https://www.bis.org/statistics/full_we… ## # … with 12 more rows For the purpose of our excercise we have choose tables 6 and 18 from the BIS data set. Table 6 corresponds to the quarterly data of the Debt service ratios for the private non-financial sector for the countries that are part of the BIS. In order to facilite our example we are going to filter our data and only keep the quaterly information that corresponds to the Private non-financial sector. Debt_service=get_bis(datasets$url[6]) ## Parsed with column specification: ## cols( ## .default = col_character() ## ) ## See spec(...) for full column specifications. ## Parsed with column specification: ## cols( ## .default = col_double(), ## FREQ = col_character(), ## Frequency = col_character(), ## BORROWERS_CTY = col_character(), ## `Borrowers&#39; country` = col_character(), ## DSR_BORROWERS = col_character(), ## Borrowers = col_character(), ## `Time Period` = col_character() ## ) ## See spec(...) for full column specifications. glimpse(Debt_service) ## Observations: 5,334 ## Variables: 8 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;BE&quot;, &quot;BE&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Belgiu… ## $ dsr_borrowers &lt;chr&gt; &quot;H&quot;, &quot;N&quot;, &quot;P&quot;, &quot;H&quot;, &quot;N&quot;, &quot;P&quot;, &quot;P&quot;, &quot;H&quot;, &quot;N&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Households &amp; NPISHs&quot;, &quot;Non-financial corporat… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value &lt;dbl&gt; 10.0, 44.3, 16.3, 6.1, 36.3, 13.8, 40.0, 10.6,… Debt_service_filter=filter(Debt_service, Debt_service$dsr_borrowers==&quot;P&quot;) glimpse(Debt_service_filter) ## Observations: 2,580 ## Variables: 8 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;S… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0… Table 18 corresponds to the quarterly data of the Property prices: long series for the countries that are part of the BIS. Property_prices=get_bis(datasets$url[18]) ## Parsed with column specification: ## cols( ## .default = col_character() ## ) ## See spec(...) for full column specifications. ## Parsed with column specification: ## cols( ## .default = col_double(), ## FREQ = col_character(), ## Frequency = col_character(), ## REF_AREA = col_character(), ## `Reference area` = col_character(), ## `Time Period` = col_character() ## ) ## See spec(...) for full column specifications. glimpse(Property_prices) ## Observations: 4,526 ## Variables: 6 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly… ## $ ref_area &lt;chr&gt; &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;… ## $ reference_area &lt;chr&gt; &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Ita… ## $ date &lt;chr&gt; &quot;1927-q1&quot;, &quot;1927-q2&quot;, &quot;1927-q3&quot;, &quot;1927-q4&quot;, &quot;1928… ## $ obs_value &lt;dbl&gt; 0.0343, 0.0342, 0.0340, 0.0339, 0.0338, 0.0336, 0… The key to joining our two tables are going to be the code of the country and the date. 16.5 5. Types of joins 16.5.1 5.1 Left_join Select all records from Table A, along with records from Table B for which the join condition is met (if at all). In our case Table A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table after the left join will keep all the records of Debt_service_filter. Additionally, we can observe that the columns where the name is the same in the tables Debt_service_filter and Property_prices appears with the suffix ‘.x’ and ‘.y’. to clarify the origin of the column. In the case of SQL, we need to highlight that the join function results in 14 columns, two more than in merge and left_join. This is explained because SQL leaves the totality of the columns while in the other two procedures the keys are not repeated. Left join functionality with dplyr. leftjoin_dplyr=left_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(leftjoin_dplyr) ## Observations: 2,580 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;S… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Qu… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzer… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA,… Left join functionality with base. leftjoin_base=merge(Debt_service_filter, Property_prices, all.x = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(leftjoin_base) ## Observations: 2,580 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q2&quot;, &quot;1999-q3&quot;, &quot;1999-q4&quot;, &quot;2… ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ obs_value.x &lt;dbl&gt; 16.3, 16.2, 16.4, 16.8, 17.3, 17.9, 17.9, 18.1… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ obs_value.y &lt;dbl&gt; 116.5571, 119.2170, 121.4370, 125.4269, 127.73… Left join functionality with sqldf. leftjoin_sqldf=sqldf(&quot;select * from Debt_service_filter LEFT JOIN Property_prices on Debt_service_filter.borrowers_cty = Property_prices.ref_area and Debt_service_filter.date = Property_prices.date&quot;) glimpse(leftjoin_sqldf) ## Observations: 2,580 ## Variables: 14 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;S… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0… ## $ freq..9 &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency..10 &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Qu… ## $ ref_area &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, NA, &quot;CA&quot;, &quot;CH&quot;, NA, NA, &quot;DE&quot;, &quot;DK&quot;… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzer… ## $ date..13 &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, NA, &quot;1999-q1&quot;, &quot;1999-q1&quot;… ## $ obs_value..14 &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA,… 16.5.2 5.2. Right_join Select all records from Table B, along with records from Table A for which the join condition is met (if at all). In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table after the right join will keep all the records of Property_prices. Right join functionality with dplyr. rightjoin_dplyr=right_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(rightjoin_dplyr) ## Observations: 4,526 ## Variables: 12 ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers_cty &lt;chr&gt; &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;, &quot;IT&quot;… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ date &lt;chr&gt; &quot;1927-q1&quot;, &quot;1927-q2&quot;, &quot;1927-q3&quot;, &quot;1927-q4&quot;, &quot;1… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;Italy&quot;, &quot;… ## $ obs_value.y &lt;dbl&gt; 0.0343, 0.0342, 0.0340, 0.0339, 0.0338, 0.0336… Right join functionality with base rightjoin_base=merge(Debt_service_filter, Property_prices, all.y = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(rightjoin_base) ## Observations: 4,526 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;… ## $ date &lt;chr&gt; &quot;1970-q1&quot;, &quot;1970-q2&quot;, &quot;1970-q3&quot;, &quot;1970-q4&quot;, &quot;1… ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ obs_value.y &lt;dbl&gt; 9.8398, 10.0197, 10.2997, 10.6197, 10.9197, 11… Right join functionality with sqldf is not supported. 16.5.3 5.3. Inner_join Select all records from Table A and Table B, where the join condition is met. In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table will keep only the rows in the intersection. Inner join functionality with dplyr. innerjoin_dplyr=inner_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(innerjoin_dplyr) ## Observations: 1,664 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 21.4, 16.8, 13.0, 21.1, 11.6, 13.3… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, 101.6600, 90.9073, 95.9194… Inner join functionality with base. innerjoin_base=merge(Debt_service_filter, Property_prices, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(innerjoin_base) ## Observations: 1,664 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q2&quot;, &quot;1999-q3&quot;, &quot;1999-q4&quot;, &quot;2… ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ obs_value.x &lt;dbl&gt; 16.3, 16.2, 16.4, 16.8, 17.3, 17.9, 17.9, 18.1… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ obs_value.y &lt;dbl&gt; 116.5571, 119.2170, 121.4370, 125.4269, 127.73… Inner join functionality with sqldf. innerjoin_sqldf=sqldf(&quot;select * from Debt_service_filter INNER JOIN Property_prices on Debt_service_filter.borrowers_cty = Property_prices.ref_area and Debt_service_filter.date = Property_prices.date &quot;) glimpse(innerjoin_sqldf) ## Observations: 1,664 ## Variables: 14 ## $ freq &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value &lt;dbl&gt; 16.3, 13.8, 21.4, 16.8, 13.0, 21.1, 11.6, 13.3… ## $ freq..9 &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency..10 &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ ref_area &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;DE&quot;, &quot;DK&quot;, &quot;ES&quot;, &quot;FI&quot;… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Canada&quot;, &quot;Switzerland… ## $ date..13 &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value..14 &lt;dbl&gt; 116.5571, 115.9600, 101.6600, 90.9073, 95.9194… 16.5.4 5.4. Full_join Select all records from Table A and Table B, where the join condition is met. In our case Table, A corresponds to Debt_service_filter and Table B to Property_prices. Notice that in our example the resulting table will keep all the rows in both tables. Full join functionality with dplyr. outerjoin_dplyr=full_join(Debt_service_filter, Property_prices, by=c(&#39;borrowers_cty&#39;=&#39;ref_area&#39;,&#39;date&#39;=&#39;date&#39;)) glimpse(outerjoin_dplyr) ## Observations: 5,442 ## Variables: 12 ## $ freq.x &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.x &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;BE&quot;, &quot;BR&quot;, &quot;CA&quot;, &quot;CH&quot;, &quot;CN&quot;, &quot;CZ&quot;, &quot;DE&quot;… ## $ borrowers_country &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, &quot;Brazil&quot;, &quot;Canada&quot;, &quot;S… ## $ dsr_borrowers &lt;chr&gt; &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;P&quot;, &quot;… ## $ borrowers &lt;chr&gt; &quot;Private non-financial sector&quot;, &quot;Private non-f… ## $ date &lt;chr&gt; &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1999-q1&quot;, &quot;1… ## $ obs_value.x &lt;dbl&gt; 16.3, 13.8, 40.0, 21.4, 16.8, 10.9, 13.8, 13.0… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, NA, &quot;Q&quot;, &quot;Q&quot;, NA, NA, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;,… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, NA, &quot;Quarterly&quot;, &quot;Qu… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Belgium&quot;, NA, &quot;Canada&quot;, &quot;Switzer… ## $ obs_value.y &lt;dbl&gt; 116.5571, 115.9600, NA, 101.6600, 90.9073, NA,… Full join functionality with base. outerjoin_base=merge(Debt_service_filter, Property_prices, all.y = TRUE, all.x = TRUE, by.x=c(&quot;borrowers_cty&quot;, &quot;date&quot;), by.y=c(&quot;ref_area&quot;, &quot;date&quot;)) glimpse(outerjoin_base) ## Observations: 5,442 ## Variables: 12 ## $ borrowers_cty &lt;chr&gt; &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;, &quot;AU&quot;… ## $ date &lt;chr&gt; &quot;1970-q1&quot;, &quot;1970-q2&quot;, &quot;1970-q3&quot;, &quot;1970-q4&quot;, &quot;1… ## $ freq.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ frequency.x &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers_country &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ dsr_borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ borrowers &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ obs_value.x &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ freq.y &lt;chr&gt; &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;Q&quot;, &quot;… ## $ frequency.y &lt;chr&gt; &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarterly&quot;, &quot;Quarte… ## $ reference_area &lt;chr&gt; &quot;Australia&quot;, &quot;Australia&quot;, &quot;Australia&quot;, &quot;Austra… ## $ obs_value.y &lt;dbl&gt; 9.8398, 10.0197, 10.2997, 10.6197, 10.9197, 11… Full join functionality with sqldf is not supported. "],
["the-first-step-to-analyse-a-dataset.html", "Chapter 17 The first step to analyse a dataset 17.1 Introduction 17.2 A glimpse at the dataset 17.3 Dive into one column 17.4 Advanced patterns about a data set", " Chapter 17 The first step to analyse a dataset Weitao Chen and Jianing Li 17.1 Introduction Oftentimes we get a large dataset to conduct data analysis or to prepare the data to be further used in our model. A common question data analysts hear is “where do I even start in my analysis?”. We will introduce serveral functions to help you take a glance at your giant dataset so you can have a fundemental undetstanding of your data. 17.2 A glimpse at the dataset 17.2.1 How does the data look like? When you come across a new dataset, you may ask: How does the dataset look like? “To see is to know. Not to see is to guess.” To answer this question, you just need to view its content. View works not only in RStudio, but also in R terminal on Windows. However, its output can’t be knitted into html or pdf. Though, when the dataset is huge, you would not really like to open and view the entire dataset. Then what? Just take a sample of it. The first function that comes to your mind? head! ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 NA 1.4 0.2 setosa ## 3 NA 3.2 1.3 0.2 setosa ## 4 NA NA 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa head returns the first 5 rows of a data frame by default. You can also specify how many rows you want. head(iris,3) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 NA 1.4 0.2 setosa ## 3 NA 3.2 1.3 0.2 setosa tail is similar to it, but it retrives the rows from the bottom. tail(iris,2) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 149 6.2 3.4 5.4 2.3 &lt;NA&gt; ## 150 5.9 3.0 5.1 1.8 virginica As you can see, we get only the flowers of species “setosa” with head and of “virginica” with tail. That’s because the original dataset is ordered by the “Species”, and its head and tail are homogeneous. What if we want to get a heterogeneous view? You may use sample_n instead. It selects random rosw from a table for you. library(dplyr) sample_n(iris, 5) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.0 3.2 1.2 0.2 setosa ## 2 4.3 3.0 1.1 0.1 setosa ## 3 6.0 2.9 4.5 1.5 versicolor ## 4 4.9 2.5 4.5 1.7 virginica ## 5 6.3 3.3 6.0 2.5 virginica When you want to take a glimpse at a huge dataset, it’s fairly better choice to use sample_n. However, unlike head, tail or View, it doesn’t work on vector or types other than data frame. Also, you have to library dplyr to use sample_n, while other 3 functions come in baser. 17.2.2 Retrive the metadata For a data frame, one of the first property we want to learn about it would be its size. dim(iris) ## [1] 150 5 The first value in the result is the number of observations and the second is of variables. Sometimes we don’t really want to know the details of a dataset. We simply need a summary. summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.00 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.60 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.30 Median :1.300 ## Mean :5.847 Mean :3.067 Mean :3.73 Mean :1.209 ## 3rd Qu.:6.400 3rd Qu.:3.400 3rd Qu.:5.10 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.90 Max. :2.500 ## NA&#39;s :8 NA&#39;s :7 NA&#39;s :5 NA&#39;s :10 ## Species ## setosa :49 ## versicolor:47 ## virginica :48 ## NA&#39;s : 6 ## ## ## When used on a data frame, summary gives you the metadata of the dataset. That is, the summaries for each vairable in it. For numerical column, you will see the percentiles and mean. For categorical ones, you can get the frequency of each factors. A simpler alternative is str. Note that str here is not an abbreviation for “string”, but for “structure”. str “compactly displays the structure of an arbitrary R object”, according to the document. As for data frame, it displays the data type and first few values of each columns. str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 NA NA 5 5.4 NA 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 NA 3.2 NA 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 NA 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 17.3 Dive into one column 17.3.1 Summarise a numerical variable If you would like to examine some columns more thoroughly, you can use summarise to customize your summary. It takes the data frame as the first parameter. Name-value pairs of summary functions are followed, indicating the output titles and content. Available summary functions are as followed: Center: mean(), median() Spread: sd(), IQR(), mad() Range: min(), max(), quantile() Position: first(), last(), nth(), Count: n(), n_distinct() Logical: any(), all() library(dplyr) iris %&gt;% summarize(mean.1 = mean(Petal.Length, na.rm=TRUE), sd.1 = sd(Petal.Length, na.rm=TRUE), mean.2 = mean(Sepal.Length, na.rm=TRUE), sd.2 = sd(Sepal.Length, na.rm=TRUE)) ## mean.1 sd.1 mean.2 sd.2 ## 1 3.729655 1.763524 5.847183 0.8046579 17.3.2 Understand a categorical variable You may learn about the frequency about a categorical column in a data frame using summary. summary(iris$Species) ## setosa versicolor virginica NA&#39;s ## 49 47 48 6 Though, it doesn’t work when the column is presented not as a factor, but as characters. iris$Characters &lt;- as.character(iris$Species) summary(iris$Characters) ## Length Class Mode ## 150 character character Under such circumstances, we may use unique and table instead. unique tells you about all the unique values in a vector, and table shows their frequency. unique(iris$Characters) ## [1] &quot;setosa&quot; NA &quot;versicolor&quot; &quot;virginica&quot; table(iris$Characters) ## ## setosa versicolor virginica ## 49 47 48 17.4 Advanced patterns about a data set 17.4.1 Locate the missing values Sometime there are missing values or NA in our dataset. It can be caused by a number of reasons such as observations that were not recorded and data corruption. Or it can be left as NA on purpose to indicate something. The important thing is how to find the missing values in our dataset. In this block we will introduce several functions to find the missing values Check missing values by columns colSums(is.na(iris)) %&gt;% sort(decreasing=TRUE) ## Petal.Width Sepal.Length Sepal.Width Species Characters ## 10 8 7 6 6 ## Petal.Length ## 5 Check missing values by row rowSums(is.na(iris)) %&gt;% sort(decreasing=TRUE) ## [1] 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 ## [36] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [71] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [106] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## [141] 0 0 0 0 0 0 0 0 0 0 Use a heatmap to check missing values library(ggplot2) tidyiris &lt;- iris %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, -id) %&gt;% mutate(missing = ifelse(is.na(value), &quot;yes&quot;, &quot;no&quot;)) ## Warning: attributes are not identical across measure variables; ## they will be dropped ggplot(tidyiris, aes(x = key, y = fct_rev(id), fill = missing)) + geom_tile(color = &quot;white&quot;) + ggtitle(&quot;iris with NAs added&quot;) + scale_fill_viridis_d() + # discrete scale theme_bw() if(&quot;mi&quot; %in% rownames(installed.packages()) == TRUE) { library(mi) x &lt;- missing_data.frame(iris) image(x) } ## NOTE: The following pairs of variables appear to have the same missingness pattern. ## Please verify whether they are in fact logically distinct variables. ## [,1] [,2] ## [1,] &quot;Species&quot; &quot;Characters&quot; ## Warning in .local(.Object, ...): Species and Characters have the same rank ordering. ## Please verify whether they are in fact distinct variables. 17.4.2 Find the outlier for numeric values An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.[3] An outlier can cause serious problems in statistical analyses. Here we find the outliers in each columns. tidyiris &lt;- clean.iris[1:4] %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, -id) ggplot(data=tidyiris)+ geom_boxplot(mapping=aes(x=key,y=value)) Thus we can see that there are 4 outliers in Sepal.Width variable. 17.4.3 Find out the correlations among variables For our numeric data, it is important to find the correlations between variables, since it provides us extra information about the dataset. library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ## ## Attaching package: &#39;GGally&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## nasa ggpairs(clean.iris[1:4]) "],
["workshops.html", "Chapter 18 Workshops 18.1 bookdown", " Chapter 18 Workshops Some groups of students have contributed to the community by running the following workshops. 18.1 bookdown Weixi Yao and Wangzhi Li This introductory workshop on bookdown is designed to give a complete guide to the bookdown package. The workshop is split into two parts. The first part covers the basic information, including what is bookdown, why use bookdown and what are the other options avaiable. The second part serves as a practice session, and each attendee will try to build their own books using the instruction we provide. If the attendees want to know more about bookdown, they can always go back to our slides for reference. We have uplodaed our complete slide to the repo (url: https://github.com/yweixi/EDAV-community-contribution.git; file name: EDAV Community Contribution.pdf). Also, all the materials we use during the workshop can be found in a seperate repo (url: https://github.com/SafeguardLi/SafeguardLi.github.io.git). #R Package Writing Siddhant Shandilya and Mohit Chander Gulla R packages are an ideal way to package and distribute R code and data for re-use by others. This workshop will provide you with an overview of how to create your own pacakge in R. The walkthrough gives step by step instructions on how to define your functions, create a project for your package, embed your functions and its documentation within it and finally how to compile and build it into an R package that is ready to be shared or published. All the materials used in the workshop can be found at: https://github.com/siddhantshandilya/EDAV---Community-Contribution-19 You may refer to the reference links provided at the end of the pdf which goes into further details on how to publish your package on CRAN repository, if you are interested. "],
["cheatsheets.html", "Chapter 19 Cheatsheets 19.1 HTML, JavaScript, and D3 19.2 Leaflet", " Chapter 19 Cheatsheets Some groups of students have contributed to the community by writing the following cheat sheets. 19.1 HTML, JavaScript, and D3 Yitao Liu (yl4343) and Yiyang Sun (ys3284) In addition to the visualization tools we have learned using R, we would like to introduce another powerful visualization tool, D3.js. We created a GitHub Page as well as the GitHub Repository to introduce basic knowledge of building a D3 visualization. We have made cheat sheets for HTML, JavaScript – two cornerstones of coding with D3 library, and a cheat sheet for D3.js. To better help fellow students to kick-off with D3 visualization, we also provided code examples of HTML and D3 in our GitHub Repository. Link to our GitHub Page: https://tonyytliu.github.io/Stat5702_CC60/ Link to our GitHub Repository: https://github.com/tonyytliu/Stat5702_CC60 19.2 Leaflet Di Ye and Qiaoge Zhu This project creates a cheatsheet on leaflet package in R. link: https://drive.google.com/file/d/1NgiAYy7kaoheWbCmJRmu9NxI6Ag7s-VT/view?usp=sharing "],
["videos.html", "Chapter 20 Videos 20.1 CartoDB", " Chapter 20 Videos Some groups of students have contributed to the community by creating video tutorials 20.1 CartoDB Luis Lu and Timothy Huang This tutorial gives a brief overview on getting started with CartoDB, a powerful cloud computing tool that provides geospatial analysis and mapping tools. In this tutorial, we will go over the steps of getting set up on CartoDB, uploading your first dataset, creating your first map visualization, and exploring a few of Carto’s provided geospatial data analysis tools. Link to tutorial video: https://www.youtube.com/watch?v=GxRRXWTMMe8&amp;feature=youtu.be CartoDB "],
["translation.html", "Chapter 21 Translation 21.1 Translation for R &amp; ggplot2 visualization tutorials", " Chapter 21 Translation Some groups of students have contributed to the community by translating useful resources into another language. 21.1 Translation for R &amp; ggplot2 visualization tutorials Yuchen Pei and Jiaqi Tang We translated two online tutorials for visualizaiton in R into Chinese. The first one is called A Comprehensive Guide to Data Visualization in R for Beginners and the second one is called ggplot2: Mastering the basics. Our translation files can be found here: https://github.com/Jasmine1231/EDAV-19Fall-Community_Contribution . "],
["list-of-community-contribution.html", "Chapter 22 List of Community Contribution", " Chapter 22 List of Community Contribution Kevin Gao (wg2311) and Haibo Yu (hy2628) We have tried many different things from the list, including: 22.0.1 * A lighting talk in class Title: Experiments Tool and Visualization Description: The “Virtual Lab” refers to using software controlled experiments with internet participants to overcome many of the limitations of traditional lab experiments. To help researchers study more complex tasks and set up interactions that happen over longer periods of time or among larger numbers of people. This allows us to design behavioral experiments and visualize data that would have been very hard to do in the past. I would like to share some of the work and how we visualize things to make a difference. 22.0.2 * A cheatsheet About mapping grammar for ggplot2,pyplot,d3 22.0.3 * A series of tutorials About how to share your R work (4 ways): Publish and share your R plots, hosted free in github website (Quick, Simple and Free) Release R Package, through cran or github Deploy R model to Azure ML Studio (The Simplest Way) Deploy R model as Web Service (The Structured Way) 22.0.4 * A workshop - “ShareYouRWork” Give a walkthough demo on how to deploy R model as web service. (The Structed Way) For more information, please visit our github main repo here "],
["hex-sticker.html", "Chapter 23 Hex Sticker", " Chapter 23 Hex Sticker Priyadharshini Rajbabu and Suman Tripathy Our group was tasked with designing a hex sticker for the EDAV course, under the advisement of Professor Robbins. We were initially provided with a folder of various logos used on the edav.info webpage. They can be found at this link https://github.com/jtr13/EDAV/tree/master/images/icons. We first considered using the hexSticker package in R to create a hex sticker design, but ran into issues with image quality on the sticker and also felt there were limited resources for design with this package. A better alternative was to use Adobe Photoshop to design the sticker with the help of a blank psd template from Sticker Mule, to set up the proper sizing and to allow for easy printing. Some things we took into consideration in detail while designing the sticker were: Color scheme of the sticker: Decided to stick to the shade of blue widely used on edav.info webpage. After much experimentation, we converted the icons and logos to an all-black theme, which better complemented the blue. Cropping the edav.info logo: We cut out the “.info” half of the initial logo for this sticker. The technique to doing this in photoshop was to erase this half and set it to a transparent background, allowing the blue background to take its place. Ordering of the three elements of the sticker: We experimented with quite a few variations of the sticker to see what layout/ordering of the edav icon, logo, and “fall 2019” title would be most compatible. The sticker has been saved in a photoshop (psd) file, as that is where we designed it. PSD FILE -&gt; https://github.com/prajbabu/edavhex/blob/master/edavhexsticker.psd For readability, and for those who don’t have Adobe Photoshop installed, the file has also been saved as a: PDF -&gt; https://github.com/prajbabu/edavhex/blob/master/edavhexstickerFINAL.pdf PNG -&gt; https://github.com/prajbabu/edavhex/blob/master/hexsticker_image.png. If hyperlinks don’t work, please copy link into a browser.* "]
]
